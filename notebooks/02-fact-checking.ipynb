{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b529140b",
   "metadata": {},
   "source": [
    "# 02 - Fact Checking\n",
    "\n",
    "This notebook is responsible for performing the fact-checking task on the claims that were extracted and normalized in the previous notebook. It loads the datasets generated previously, creates batches of jobs for fact-checking, and processes these jobs so that the LLM can classify the claims as true or false."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871cee1f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35653a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Native\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Third-party\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Local\n",
    "from shared.prompts import (\n",
    "    FACT_CHECKING_SYSTEM_MESSAGE,\n",
    ")\n",
    "from shared.utils import move_file_to_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d77615",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60919a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519f4488",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ef19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATASET = \"faketweetbr\"  # Dataset that will be used to generate new datasets after claim extraction and normalization tasks.\n",
    "MODEL = \"gpt-5\"  # Model that will be used to generate the datasets.\n",
    "DATASET_IMPORTANT_COLUMNS = [\n",
    "    \"text\",\n",
    "    \"classificacao\",\n",
    "]  # All important columns in the dataset except index/id cloumns (custom id will be added automatically)\n",
    "PROCESS_TIME = None  # This variable is used to resume the processing of a previous execution. If None, current timestamp will be used.\n",
    "PROCESS_ID = f\"{MODEL}_{PROCESS_TIME if PROCESS_TIME else pd.Timestamp.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "# Directory paths\n",
    "DATASET_PATH = f\"../data/{DATASET}\"\n",
    "ORIGINAL_DATASET_PATH = DATASET_PATH + \"/original\"\n",
    "UNPROCESSED_BATCHES_DIR = f\"{DATASET_PATH}/batches/unprocessed/{PROCESS_ID}\"\n",
    "UPLOADED_BATCHES_DIR = f\"{DATASET_PATH}/batches/uploaded/{PROCESS_ID}\"\n",
    "PROCESSING_BATCHES_DIR = f\"{DATASET_PATH}/batches/processing/{PROCESS_ID}\"\n",
    "PROCESSED_BATCHES_DIR = f\"{DATASET_PATH}/batches/processed/{PROCESS_ID}\"\n",
    "RESULTS_BATCHES_DIR = f\"{DATASET_PATH}/batches/results/{PROCESS_ID}\"\n",
    "FAILED_BATCHES_DIR = f\"{DATASET_PATH}/batches/failed/{PROCESS_ID}\"\n",
    "\n",
    "# OpenAI batch processing parameters\n",
    "COMPLETION_ENDPOINT = \"/v1/chat/completions\"\n",
    "MAX_COMPLETION_TOKENS = None\n",
    "TEMPERATURE = 1  # Obs: gpt-5-nano does not support temperature=0\n",
    "VERBOSITY = \"low\"  # Options: \"low\", \"medium\", \"high\"\n",
    "REASONING_EFFORT = \"high\"  # Options: \"low\", \"medium\", \"high\"\n",
    "ROWS_PER_BATCH = 5000  # Number of rows to process in each batch (will generate double the amount of calls due to executing two tasks: claim extraction and claim normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d0a7c5",
   "metadata": {},
   "source": [
    "### Create Auxiliary Directories for Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fce490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure all necessary directories exist\n",
    "os.makedirs(UNPROCESSED_BATCHES_DIR, exist_ok=True)\n",
    "os.makedirs(UPLOADED_BATCHES_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSING_BATCHES_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_BATCHES_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_BATCHES_DIR, exist_ok=True)\n",
    "os.makedirs(FAILED_BATCHES_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c65df1",
   "metadata": {},
   "source": [
    "### Load Claim Extraction and Normalization Tasks Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0816a609",
   "metadata": {},
   "source": [
    "### Build Fact Checking Jobs Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1408c5",
   "metadata": {},
   "source": [
    "### Create Fact Checking Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54848a99",
   "metadata": {},
   "source": [
    "### Check Fact Checking Jobs Progression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df8939",
   "metadata": {},
   "source": [
    "### Save Fact Checking Jobs Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
