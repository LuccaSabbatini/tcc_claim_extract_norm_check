{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fd3d139",
   "metadata": {},
   "source": [
    "# 01 - Generate Datasets\n",
    "\n",
    "This notebook is responsible for generating two new datasets from an original dataset. The process consists on passing the original dataset through two tasks: claim extraction and claim normalization. The outputs of each task is the saved to be used in the next steps of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e669923",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9c5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Native\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Third-party\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Local\n",
    "from shared.prompts import (\n",
    "    CLAIM_EXTRACTION_SYSTEM_MESSAGE,\n",
    "    CLAIM_NORMALIZATION_SYSTEM_MESSAGE,\n",
    "    CLAIM_EXTRACTION_NORMALIZATION_SYSTEM_MESSAGE,\n",
    ")\n",
    "from shared.utils import move_file_to_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c832ba5",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "751b327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ce408d",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATASET = \"fakebr\" # Dataset that will be used to generate new datasets after claim extraction and normalization tasks.\n",
    "MODEL = \"gpt-5-nano\" # Model that will be used to generate the datasets.\n",
    "DATASET_IMPORTANT_COLUMNS = [\"text\", \"classificacao\"] # All important columns in the dataset except index/id cloumns (custom id will be added automatically)\n",
    "PROCESS_TIME = \"2025-11-06_17-58-17\"  # This variable is used to resume the processing of a previous execution. If None, current timestamp will be used (new execution).\n",
    "PROCESS_ID = f\"{MODEL}_{PROCESS_TIME if PROCESS_TIME else pd.Timestamp.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "TASKS = [\n",
    "  \"claim_extraction\", # Task to extract claims from the original text.\n",
    "  \"claim_normalization\", # Task to normalize the extracted claims.\n",
    "  # \"claim_extraction_normalization\" # Task to normalize previously extracted claims.\n",
    "] # Tasks to be executed in the dataset generation job.\n",
    "\n",
    "# Directory paths and files\n",
    "DATASET_PATH = f\"../data/{DATASET}\"\n",
    "ORIGINAL_DATASET_PATH = DATASET_PATH + \"/original\"\n",
    "ORIGINAL_DATASET_FILES = [f\"{ORIGINAL_DATASET_PATH}/train.csv\", f\"{ORIGINAL_DATASET_PATH}/test.csv\"]\n",
    "UNPROCESSED_BATCHES_DIR = f\"{DATASET_PATH}/batches/datasets_generation_jobs/unprocessed/{PROCESS_ID}\"\n",
    "UPLOADED_BATCHES_DIR = f\"{DATASET_PATH}/batches/datasets_generation_jobs/uploaded/{PROCESS_ID}\"\n",
    "PROCESSING_BATCHES_DIR = f\"{DATASET_PATH}/batches/datasets_generation_jobs/processing/{PROCESS_ID}\"\n",
    "PROCESSED_BATCHES_DIR = f\"{DATASET_PATH}/batches/datasets_generation_jobs/processed/{PROCESS_ID}\"\n",
    "RESULTS_BATCHES_DIR = f\"{DATASET_PATH}/batches/datasets_generation_jobs/results/{PROCESS_ID}\"\n",
    "FAILED_BATCHES_DIR = f\"{DATASET_PATH}/batches/datasets_generation_jobs/failed/{PROCESS_ID}\"\n",
    "\n",
    "# OpenAI batch processing parameters\n",
    "COMPLETION_ENDPOINT = \"/v1/chat/completions\"\n",
    "MAX_COMPLETION_TOKENS = None\n",
    "TEMPERATURE = 1 # Obs: gpt-5-nano does not support temperature=0\n",
    "VERBOSITY = \"low\" # Options: \"low\", \"medium\", \"high\"\n",
    "REASONING_EFFORT = \"high\" # Options: \"low\", \"medium\", \"high\"\n",
    "ROWS_PER_BATCH = 5000 # Number of rows to process in each batch (will generate double the amount of calls due to executing two tasks: claim extraction and claim normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bcdc50",
   "metadata": {},
   "source": [
    "### Create Auxiliary Directories for Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55835f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure all necessary directories exist\n",
    "os.makedirs(UNPROCESSED_BATCHES_DIR, exist_ok=True)\n",
    "os.makedirs(UPLOADED_BATCHES_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSING_BATCHES_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_BATCHES_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_BATCHES_DIR, exist_ok=True)\n",
    "os.makedirs(FAILED_BATCHES_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b3cb4e",
   "metadata": {},
   "source": [
    "### Load Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d745291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 18:00:24,796 - INFO - Total records: 7200\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "classificacao",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "custom_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "84bbc059-d7c8-48d6-b917-61657cb2f565",
       "rows": [
        [
         "0",
         "dia plateia domingao precisou atendimento medico apos consumir carne estragada integrantes auditorio programa domingao faustao precisaram atendimento medico apos almocarem restaurante south s place steakehouse cidade sao paulo participantes caravana disseram situacao ocorreu porque diversas pessoas consumiram carnes estragadas informacao jornal dia caso aconteceu ultimo domingo cerca pessoas estariam participando almoco fazia parte producao programa acordo colunista leo dias proprias pessoas passaram mal entraram contato relatar caso apos ocorrido proprio domingo encaminhadas centro medico globo responsavel restaurante disse desconhece fato informou cozinha passa frequentes vistorias",
         "fake",
         "fakebr_train_0",
         "train"
        ],
        [
         "1",
         "unica ong df top epoca abrace ajuda mil pacientes cancer leia entrevista familias tambem recebem assistencia social qualificacao acolhimento associacao nao conta dinheiro publico grupo uniu governo construir hospital crianca polemica nao ver diz ilda anos funcionamento cerca mil familias assistidas simultaneamente unica ong distrito federal entrar lista melhores brasil divulgada revista epoca instituto doar nao recebe dinheiro publico tempos crise aperto orcamento brasileiros abrace conta cerca mil contribuidores alem parceiros empresariais manter projeto cada ano associacao recebe cerca novos pacientes diagnosticados cancer despede outros tantos recebem cura nao resistem tratamentos assistencia estende familias desses pacientes instituicao calcula cerca mil pessoas abracadas acoes assistencia social qualificacao profissional adultos acolhimento cuidados paliativos outras pra gente premiacao epoca grande valor nao monetario passaporte significa legitimidade eficiencia area saude diz presidente abrace ilda peliz comandou associacao ultimos anos apos perder propria filha anos tumor sistema nervoso ilda afirma todo trabalho abrace prestado sociedade instituicao tambem encarrega cobrar deveres estado relacao pacientes problema segundo acoes sao limitadas frentes desafios diagnostico cancer exemplo estado garante tratamento medico crianca chegar casa dormir chao tres irmaos dividindo pedaco espuma abrace vai colocar cama paciente banheiro nessa casa garantir geladeira energia eletrica armazenar medicamento afirma assim acontece rede publica tradicional df abrace forcada demanda atravessar fronteiras capital ilda afirma brasilia polo convergente tratamento cancer recebe pacientes norte nordeste estados goias minas gerais nestes casos apoio familias ainda abrangente inclui hospedagem temporaria orientacao ate lobby junto governos locais imagine crianca interior amazonas venha brasilia tratamento tumor cerebro dor cabeca prefeito paga passagem barco onibus abrace vai tentar pagar passagem aerea nao topar abrace vai tentar pagar passagem ilda sao historias tocam coracao apoiadores associacao historias familia amazonia frente morte crianca cancer lutar conseguir enviar corpo volta cidade origem contribuicao doadores traslado nao sido feito familia perdido direito velorio digno abrace inaugurou df maior obra desses anos funcionamento hospital crianca jose alencar primeiro bloco funcionamento ha seis anos construido repasses secretaria saude ministerio saude sociedade civil segundo ainda construcao resultou convenio governo df ong internacional milhoes atendimentos curriculo ate junho deste ano hospital viu centro polemica durante gestao rodrigo rollemberg governo usou centro saude exemplo boa gestao propor expansao organizacoes sociais saude pubica proposta rechacada pente fino gerencia unidade icipe revelou serie inconsistencias colocando todo modelo sob duvida ilda disse acreditar investigacoes nao atingiram imagem hospital abrace porque sao instituicoes diferentes icipe recebe recursos estado administrar hospital abrace nao gente lamenta crise coisas estao caminhando dificil voce querer fazer trabalho passar conduta desse trabalho guerra politica movimento politico perde crianca porque ha desgaste pessoas estao ali atuando diz presidente",
         "true",
         "fakebr_train_1",
         "train"
        ],
        [
         "2",
         "renan parte pra guerra juizeco primeira instancia nao pode atentar contra poderes renan calheiros irritado correcao renan c hamou juizeco juiz vara federal vallisney souza oliveira responsavel operacao metis prendeu chefe policia legislativa juizeco primeira instancia nao pode atentar contra poderes ministro justica nao portado ministro estado maximo portado chefete policia renan desesperado chefe policia legislativa preso atuava capanga senado vai falar tudo eduardo cunha tambem promete jogar email protected ventilador ate momento presidente senado alvo inqueritos stf tambem aparece audios gravados sergio machado transpetro criticando operacao lava jato xingando procurador geral republica mau carater renan calheiros declarando guerra contra todos poderes vai perder leia tambem",
         "fake",
         "fakebr_train_2",
         "train"
        ],
        [
         "3",
         "presidente afastada dilma rousseff tentou condenacao anunciada discursar senado federal propria defesa recorrendo nao eventuais virtudes obvias deficiencias falta eloquencia confusao mental concluiu discurso apelando metafora grandiloquente totalmente irrealista dizer democracia brasileira sentada lado banco reus terminasse condenada tal governo povo povo povo acordo antigos atenienses sofreria sido condenada decretacao morte politica tristes tropicos parece ate imagem bonita horrenda pois implica negacao verdade unica garantia legalidade legitimidade qualquer pacto usar palavra predilecao especial pessoal familiar social economico politico faltar verdade implica quebrar qualquer acordo qualquer natureza sra rousseff mentiu primeira ultima palavra discurso montado metodo comodo desonesto copia cola aplicado espertalhoes esconder malfeitos exagerar eventuais conquistas presidente afastada recorre facilitario copia cola redatores preguicosos desde priscas eras joaozinho idolo gremio futebol vez vitima dessa pratica ainda tempos trabalhava assessoria bancada parlamentar pdt brizolista assembleia legislativa rio grande sul encarregada preparar discurso ler tribuna entregou deputado texto ja havia sido lido antes outro membro bancada atento rabugento setorista percebeu denunciou plagio jornal chamada chefe explicar estilo voce quer queime bestunto escrever texto original idiota defesa perante senadores manha copiou colou varios textos preparados mesma aplicada joaozinho nao propriamente autoplagio pois verdade autores devem ser ignotos servicais contudo menor pecados encenacao empreendeu nao mudar algum voto salvasse ser expelida baralho sair bem diante camaras anna muylaert petra costa inauguram genero documentario ficcao cinema registro golpe parlamentar manso branco denunciado esquerda dilma comecou desfiar lorotario particular dizendo corar chorar embora tentado inutilmente duas vezes sempre acreditou estado democratico direito desde amestrada caudilho socialismo moreno adoracao leonel brizola repetido refrao ligar significam estado democracia direito nao ser sempre mentindo relatar dores infortunios ser submetida tortura poroes ditadura virada anos seculo passado afinal militante grupo armado objetivo precipuo substituir uso armas sequestros assaltos mortes violencia outra inspirada tiranos brutais stalin mao pol pot last but not least idolos carteirinha fidel raul castro nao mudei lado continuei lutando democracia completou reforcar conviccao citou passado recente ameacas democracia passagem inspiracao burguesa anos brasil comparou adversarios inimigos levaram getulio vargas suicidio verdade penosa longa leitura diarios ditador estado novo publicados neta celina vargas amaral peixoto franco permite qualquer leitor atento perceber primeiro patricio ser alcunhado pai pobres suicida vocacional releitura heterodoxa historia recente brasil dilma ainda lembrou rarissimo lampejo fugaz lucidez militares republica galeao artifices golpes malsucedidos aragarcas impedir posse juscelino cancelamento posse vice joao chamou vicente goulart apos renuncia janio seguida imposicao aceitou solucao parlamentarista impasse politico tentativas abortadas enfim dez anos terem tentado vao depor gege imerso mar lama comparado traquinagens pt dilma nao passava poca militares empolgaram poder impuseram interrupcao democracia lembranca gaucho bonachao deixou seduzir charme reformas base esquerda derrubado levou comparacao momento atual rara impropriedade academica adotada respeitada filosofa petista usp marilena chaui principio cautelosa dilma denunciou risco ruptura democratica avancando ate denuncia explicita golpe medida distanciava texto escrito deixava conduzir propria dislexia lembrou senadora ana amelia lemos contudo presenca mesa senado ocupando dia tempo juizes naturais atencao ansiosa angustiada nacao desautoriza hipotese nexo posse exemplar constituicao vigente parlamentar reduziu mantra arrevesado po nao impediu muitas outras vezes espectador atento acompanhou longo sessao historica submetido tatibitate fantasia histerica surrealista tudo disse repetiu possivel perceber apenas narrativa estapafurdia conspiracao elites politicos corruptos sequiosos poder derrotados eleicao montaram impeachment participacao procurador tribunal contas uniao julio marcelo oliveira inestimavel adesao eduardo cunha entao presidente camara mero preconceito misogino ai pecado original capaz acreditar eventualidade todos ministros tcu terem sido convencidos nao contou procurador deputados terem sido submetidos vontade tiranica presidente casa aprovando so tornar desgoverno inviavel impeachment inevitavel comissao especial composta interferencia favor parte stf perdeu plenario camara alem duas vezes plenario senado outra defesa oral durante impos graves lesoes gramatica portuguesa inventou inexistente eleicao indireta vice votos michel temer lugar verdade mesma forma todos deputados senadores autorizaram julgamento pronunciaram agora julgam eleito caso temer reeleito numero votos fato ter sidi parceiro chapa muitos combativos defensores tentou atacando oliveira cunha temer nao sao personagens julgamento impeachment balela outra excelencia cometeu sentenca bombastica acumplicia ilicito nao merece governar pais ninguem ilusao confissao nada disso resposta senador aloysio nunes ferreira dilma disse tendo cumprido rigorosamente todas formalidades exigidas stf impeachment ilegitimo porque nao cumpre dever elementar justica considerar fato real nao ritos juridicos conclusao obvia condenarem tcu camara senado stf aplicado golpe rasteiro senado salvara democracia perigo pais diria tia esquizofrenica madama nao menor senso locao jornalista poeta escritor",
         "true",
         "fakebr_train_3",
         "train"
        ],
        [
         "4",
         "coreia norte critica sancoes disposta dialogar eua horas antes admitir possibilidade negociar pyongyang classificou embargos americanos empresas facilitam comercio guerra impasses questoes essenciais podem inviabilizar reuniao pyeongchang tres dias estados unidos aplicarem severas sancoes coreia norte pais asiatico declarou neste domingo estar disposto dialogar americanos recado transmitido general kim lider delegacao pais cerimonia encerramento jogos olimpicos pyeongchang coreia sul eua consideram proposta dialogo coreia norte passo desnuclearizacao horas antes coreia norte havia classificado ato guerra novo pacote sancoes imposto eua contra empresas maritimas facilitam transporte produtos pyongyang ameacou retaliar eua pais ousadia enfrentar coreia norte forma severa menos tres horas kim militares leais lider kim chegou pais vizinho cerimonia encerramento encontro presidente moon presidente moon enfatizou dialogo eua coreia norte deve ocorrer rapido possivel disse presidencia kim delegacao norte transmitiu desejo lider kim fazer eua estudam ampliar interceptacao navios suspeitos violar sancoes coreia norte apesar cancelamento cima hora encontro americano mike pence principais autoridades pyongyang washington post informou domingo negociadores dois paises podem ter conversado durante jogos olimpicos pyeongchang americana allison hooker veterana funcionaria conselho seguranca nacional eua especialista coreia norte acompanhou filha donald trump ivanka trump lider delegacao americana olimpiada coreia norte enviou jogos choe principal negociador assuntos relacionados eua ministerio relacoes exteriores washington post afirma dois reuniram secretamente durante cerimonia encerramento jogos domingo objetivo pavimentar encontros futuros dois ja negociaram juntos outras ocasioes eua punem empresas ajudam coreia norte burlar sancoes futuro negociacoes analistas ouvidos imprensa americana afirmam dialogos alto nivel eua coreia norte sao dificeis porque ambos lados permanecem inflexiveis reuniao coreia norte exige eua interrompam exercicios militares coreia sul destes treinamentos vai ocorrer abril pyongyang tambem quer continuar programa nuclear enquanto eua exigem desmantelamento outro problema resiliencia ha anos regime kim brinca gato rato ocidente vezes negociando vezes rejeitando conversas enquanto desenvolve programa nuclear trump ameaca coreia norte fase dois caso sancoes falhem hoje ha impasse diplomatico fim vista pior provavel abertura possa descarrilar facilmente coreia norte testar algum missil eua coreia sul realizarem novos exercicios militares afirmou site vox mira professora estudos asiaticos universidade yale situacao atual diplomacia ainda vale pena so reduzir risco erro calculo pode provocar guerra regional afp ap nyt",
         "true",
         "fakebr_train_4",
         "train"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>classificacao</th>\n",
       "      <th>custom_id</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dia plateia domingao precisou atendimento medi...</td>\n",
       "      <td>fake</td>\n",
       "      <td>fakebr_train_0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unica ong df top epoca abrace ajuda mil pacien...</td>\n",
       "      <td>true</td>\n",
       "      <td>fakebr_train_1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>renan parte pra guerra juizeco primeira instan...</td>\n",
       "      <td>fake</td>\n",
       "      <td>fakebr_train_2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>presidente afastada dilma rousseff tentou cond...</td>\n",
       "      <td>true</td>\n",
       "      <td>fakebr_train_3</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coreia norte critica sancoes disposta dialogar...</td>\n",
       "      <td>true</td>\n",
       "      <td>fakebr_train_4</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text classificacao  \\\n",
       "0  dia plateia domingao precisou atendimento medi...          fake   \n",
       "1  unica ong df top epoca abrace ajuda mil pacien...          true   \n",
       "2  renan parte pra guerra juizeco primeira instan...          fake   \n",
       "3  presidente afastada dilma rousseff tentou cond...          true   \n",
       "4  coreia norte critica sancoes disposta dialogar...          true   \n",
       "\n",
       "        custom_id source  \n",
       "0  fakebr_train_0  train  \n",
       "1  fakebr_train_1  train  \n",
       "2  fakebr_train_2  train  \n",
       "3  fakebr_train_3  train  \n",
       "4  fakebr_train_4  train  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty DataFrame to hold the concatenated dataset\n",
    "dataset_df = pd.DataFrame()\n",
    "\n",
    "# Read all CSV files in the dataset directory and concatenate them into a single DataFrame\n",
    "for file in ORIGINAL_DATASET_FILES:\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        # Row source\n",
    "        row_source = file.split(\"/\")[-1].replace(\".csv\", \"\")  # e.g., train or test\n",
    "\n",
    "        # If the custom_id column does not exist, create it and save the updated dataframe back to the csv file\n",
    "        if \"custom_id\" not in df.columns:\n",
    "            df[\"custom_id\"] = (\n",
    "                DATASET + \"_\" + row_source + \"_\" + df.index.astype(str)\n",
    "            )  # Add a custom_id column to keep track of original row positions\n",
    "\n",
    "            # Put the custom_id column at the front\n",
    "            cols = df.columns.tolist()\n",
    "            cols = [cols[-1]] + cols[:-1]  # Move custom_id to the front\n",
    "            df = df[cols]\n",
    "\n",
    "            # Save the updated dataframe back to the csv file\n",
    "            df.to_csv(file, index=False)\n",
    "\n",
    "        # Add a source column to identify the origin of each row\n",
    "        df[\"source\"] = row_source\n",
    "\n",
    "        # Concatenate the current dataframe to the main dataset dataframe\n",
    "        dataset_df = pd.concat([dataset_df, df], ignore_index=True)\n",
    "\n",
    "# Keep only relevant columns\n",
    "dataset_df = dataset_df[DATASET_IMPORTANT_COLUMNS + [\"custom_id\", \"source\"]] # Add custom id and source columns to keep track of original rows\n",
    "\n",
    "# Display the first few rows of the concatenated dataset\n",
    "logging.info(f\"Total records: {len(dataset_df)}\")\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4b5af4",
   "metadata": {},
   "source": [
    "### Create Tasks Jobs Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b1590c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 18:00:25,254 - INFO - Saved batch to ../data/fakebr/batches/datasets_generation_jobs/unprocessed/gpt-5-nano_2025-11-06_18-00-24/batch_1_gpt-5-nano_2025-11-06_18-00-24.jsonl\n",
      "2025-11-06 18:01:23,882 - INFO - HTTP Request: POST https://api.openai.com/v1/files \"HTTP/1.1 200 OK\"\n",
      "2025-11-06 18:01:23,885 - INFO - Uploaded batch to OpenAI successfully! File ID: file-F1EudvyJsmhkyMKmKVrw3M\n",
      "2025-11-06 18:01:23,886 - INFO - Moved file to ../data/fakebr/batches/datasets_generation_jobs/uploaded/gpt-5-nano_2025-11-06_18-00-24/batch_1_file-id_file-F1EudvyJsmhkyMKmKVrw3M.jsonl\n",
      "2025-11-06 18:01:24,499 - INFO - HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "2025-11-06 18:01:24,500 - INFO - Created batch successfully! Batch ID: batch_690d0ca441388190a5ccf76c01a7a0bc\n",
      "2025-11-06 18:01:24,501 - INFO - Moved file to ../data/fakebr/batches/datasets_generation_jobs/processing/gpt-5-nano_2025-11-06_18-00-24/batch_1_id_690d0ca441388190a5ccf76c01a7a0bc.jsonl\n",
      "2025-11-06 18:01:24,695 - INFO - Saved batch to ../data/fakebr/batches/datasets_generation_jobs/unprocessed/gpt-5-nano_2025-11-06_18-00-24/batch_2_gpt-5-nano_2025-11-06_18-00-24.jsonl\n",
      "2025-11-06 18:01:53,304 - INFO - HTTP Request: POST https://api.openai.com/v1/files \"HTTP/1.1 200 OK\"\n",
      "2025-11-06 18:01:53,305 - INFO - Uploaded batch to OpenAI successfully! File ID: file-WNc9af2oZrT8E7dmeX47pT\n",
      "2025-11-06 18:01:53,306 - INFO - Moved file to ../data/fakebr/batches/datasets_generation_jobs/uploaded/gpt-5-nano_2025-11-06_18-00-24/batch_2_file-id_file-WNc9af2oZrT8E7dmeX47pT.jsonl\n",
      "2025-11-06 18:01:54,044 - INFO - HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "2025-11-06 18:01:54,046 - INFO - Created batch successfully! Batch ID: batch_690d0cc1bf748190b986c065565d686b\n",
      "2025-11-06 18:01:54,046 - INFO - Moved file to ../data/fakebr/batches/datasets_generation_jobs/processing/gpt-5-nano_2025-11-06_18-00-24/batch_2_id_690d0cc1bf748190b986c065565d686b.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Function to generate JSONL rows for a batch\n",
    "def generate_batch_jsonl_rows(batch_df):\n",
    "    batch_jsonl_rows = []\n",
    "\n",
    "    for _, row in batch_df.iterrows():\n",
    "        if \"claim_extraction\" in TASKS:\n",
    "            claim_extraction_row_dict = {\n",
    "\t\t\t\t\t\t\t\t\"custom_id\": f\"{row['custom_id']}_extr\",\n",
    "\t\t\t\t\t\t\t\t\"method\": \"POST\",\n",
    "\t\t\t\t\t\t\t\t\"url\": COMPLETION_ENDPOINT,\n",
    "\t\t\t\t\t\t\t\t\"body\": {\n",
    "\t\t\t\t\t\t\t\t\t\t\"model\": MODEL,\n",
    "\t\t\t\t\t\t\t\t\t\t\"messages\": [\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t{\"role\": \"developer\", \"content\": CLAIM_EXTRACTION_SYSTEM_MESSAGE},\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"role\": \"user\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"content\": f\"Postagem: {row['text']}\\nDeclaração extraída:\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t},\n",
    "\t\t\t\t\t\t\t\t\t\t],\n",
    "\t\t\t\t\t\t\t\t\t\t\"max_completion_tokens\": MAX_COMPLETION_TOKENS,\n",
    "\t\t\t\t\t\t\t\t\t\t\"metadata\": {\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\"custom_id\": row[\"custom_id\"],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\"source\": row[\"source\"],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\"classificacao\": row[\"classificacao\"],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\"task\": \"claim_extraction\",\n",
    "\t\t\t\t\t\t\t\t\t\t},\n",
    "\t\t\t\t\t\t\t\t\t\t\"verbosity\": VERBOSITY,\n",
    "\t\t\t\t\t\t\t\t\t\t\"reasoning_effort\": REASONING_EFFORT,\n",
    "\t\t\t\t\t\t\t\t\t\t\"temperature\": TEMPERATURE,\n",
    "\t\t\t\t\t\t\t\t\t\t\"store\": True,\n",
    "\t\t\t\t\t\t\t\t},\n",
    "\t\t\t\t\t\t}\n",
    "\n",
    "            batch_jsonl_rows.append(claim_extraction_row_dict)\n",
    "\n",
    "        if \"claim_normalization\" in TASKS:\n",
    "            claim_normalization_row_dict = {\n",
    "\t\t\t\t\t\t\t\t\"custom_id\": f\"{row['custom_id']}_norm\",\n",
    "\t\t\t\t\t\t\t\t\"method\": \"POST\",\n",
    "\t\t\t\t\t\t\t\t\"url\": COMPLETION_ENDPOINT,\n",
    "\t\t\t\t\t\t\t\t\"body\": {\n",
    "\t\t\t\t\t\t\t\t\t\t\"model\": MODEL,\n",
    "\t\t\t\t\t\t\t\t\t\t\"messages\": [\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"role\": \"developer\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"content\": CLAIM_NORMALIZATION_SYSTEM_MESSAGE,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t},\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"role\": \"user\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"content\": f\"Postagem: {row['text']}\\nDeclaração normalizada:\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t},\n",
    "\t\t\t\t\t\t\t\t\t\t],\n",
    "\t\t\t\t\t\t\t\t\t\t\"max_completion_tokens\": MAX_COMPLETION_TOKENS,\n",
    "\t\t\t\t\t\t\t\t\t\t\"metadata\": {\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\"custom_id\": row[\"custom_id\"],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\"source\": row[\"source\"],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\"classificacao\": row[\"classificacao\"],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\"task\": \"claim_normalization\",\n",
    "\t\t\t\t\t\t\t\t\t\t},\n",
    "\t\t\t\t\t\t\t\t\t\t\"verbosity\": VERBOSITY,\n",
    "\t\t\t\t\t\t\t\t\t\t\"reasoning_effort\": REASONING_EFFORT,\n",
    "\t\t\t\t\t\t\t\t\t\t\"temperature\": TEMPERATURE,\n",
    "\t\t\t\t\t\t\t\t\t\t\"store\": True,\n",
    "\t\t\t\t\t\t\t\t},\n",
    "\t\t\t\t\t\t}\n",
    "\n",
    "            batch_jsonl_rows.append(claim_normalization_row_dict)\n",
    "\n",
    "        if \"claim_extraction_normalization\" in TASKS:\n",
    "            claim_extraction_normalization_row_dict = {\n",
    "                \"custom_id\": f\"{row['custom_id']}_extrnorm\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": COMPLETION_ENDPOINT,\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"developer\",\n",
    "                            \"content\": CLAIM_EXTRACTION_NORMALIZATION_SYSTEM_MESSAGE,\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": f\"Postagem: {row['text']}\\nDeclaração normalizada:\",\n",
    "                        },\n",
    "                    ],\n",
    "                    \"max_completion_tokens\": MAX_COMPLETION_TOKENS,\n",
    "                    \"metadata\": {\n",
    "                        \"custom_id\": row[\"custom_id\"],\n",
    "                        \"source\": row[\"source\"],\n",
    "                        \"classificacao\": row[\"classificacao\"],\n",
    "                        \"task\": \"claim_extraction_normalization\",\n",
    "                    },\n",
    "                    \"verbosity\": VERBOSITY,\n",
    "                    \"reasoning_effort\": REASONING_EFFORT,\n",
    "                    \"temperature\": TEMPERATURE,\n",
    "                    \"store\": True,\n",
    "                },\n",
    "            }\n",
    "\n",
    "            batch_jsonl_rows.append(claim_extraction_normalization_row_dict)\n",
    "\n",
    "    return batch_jsonl_rows\n",
    "\n",
    "\n",
    "# Function to save JSONL file\n",
    "def save_batch_jsonl_file(batch_jsonl_rows, batch_file_path):\n",
    "    try:\n",
    "        with open(batch_file_path, \"w\") as jsonl_file:\n",
    "            for row in batch_jsonl_rows:\n",
    "                jsonl_file.write(\n",
    "                    json.dumps(row) + \"\\n\"\n",
    "                )  # Use json.dumps to format with double quotes\n",
    "        logging.info(f\"Saved batch to {batch_file_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving batch to {batch_file_path}: {e}\")\n",
    "\n",
    "\n",
    "# Function to upload file to OpenAI\n",
    "def upload_batch_file_to_openai(batch_file_path):\n",
    "    try:\n",
    "        batch_uploaded_file = client.files.create(\n",
    "            file=open(batch_file_path, \"rb\"), purpose=\"batch\"\n",
    "        )\n",
    "        logging.info(f\"Uploaded batch to OpenAI successfully! File ID: {batch_uploaded_file.id}\")\n",
    "        return batch_uploaded_file.id\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error uploading batch to OpenAI: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to create a batch in OpenAI\n",
    "def create_openai_batch(batch_input_file_id):\n",
    "    try:\n",
    "        batch_info = client.batches.create(\n",
    "            input_file_id=batch_input_file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\"description\": f\"Batch created from file ID {batch_input_file_id}\"},\n",
    "        )\n",
    "        logging.info(f\"Created batch successfully! Batch ID: {batch_info.id}\")\n",
    "        return batch_info.id\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating batch: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main batch processing loop\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "current_batch = 1\n",
    "\n",
    "for i in range(0, len(dataset_df), ROWS_PER_BATCH):\n",
    "    # Generate JSONL rows for the current batch\n",
    "    batch_df = dataset_df.iloc[i:i + ROWS_PER_BATCH]\n",
    "    batch_jsonl_rows = generate_batch_jsonl_rows(batch_df)\n",
    "\n",
    "    # Save the batch to a JSONL file\n",
    "    batch_file_name = f\"batch_{current_batch}_{PROCESS_ID}.jsonl\"\n",
    "    batch_file_path = f\"{UNPROCESSED_BATCHES_DIR}/{batch_file_name}\"\n",
    "    save_batch_jsonl_file(batch_jsonl_rows, batch_file_path)\n",
    "\n",
    "    # Upload the batch file to OpenAI\n",
    "    batch_input_file_id = upload_batch_file_to_openai(batch_file_path)\n",
    "\n",
    "    if batch_input_file_id:\n",
    "        # Move the batch file to the uploaded directory\n",
    "        uploaded_file_path = f\"{UPLOADED_BATCHES_DIR}/batch_{current_batch}_file-id_{batch_input_file_id}.jsonl\"\n",
    "        move_file_to_directory(batch_file_path, uploaded_file_path)\n",
    "\n",
    "        # Create the batch in OpenAI\n",
    "        batch_id = create_openai_batch(batch_input_file_id)\n",
    "\n",
    "        if batch_id:\n",
    "            # Move the batch file to the processing directory\n",
    "            processing_file_path = f\"{PROCESSING_BATCHES_DIR}/batch_{current_batch}_id_{batch_id.replace(\"batch_\", \"\")}.jsonl\"\n",
    "            move_file_to_directory(uploaded_file_path, processing_file_path)\n",
    "\n",
    "    # Increment the batch counter\n",
    "    current_batch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28dfbbe",
   "metadata": {},
   "source": [
    "### Check Batches Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bf3068d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 22:23:15,296 - INFO - HTTP Request: GET https://api.openai.com/v1/batches/batch_690d0c280b7c819090dee079ec9d69fd \"HTTP/1.1 401 Unauthorized\"\n",
      "2025-11-06 22:23:15,297 - ERROR - Error retrieving batch 690d0c280b7c819090dee079ec9d69fd: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************kQcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}\n",
      "2025-11-06 22:23:15,460 - INFO - HTTP Request: GET https://api.openai.com/v1/batches/batch_690d0c440c2081909b4438086270337d \"HTTP/1.1 401 Unauthorized\"\n",
      "2025-11-06 22:23:15,461 - ERROR - Error retrieving batch 690d0c440c2081909b4438086270337d: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************kQcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}\n"
     ]
    }
   ],
   "source": [
    "# Function to retrieve batch status from OpenAI\n",
    "def retrieve_batch_status(batch_id):\n",
    "    try:\n",
    "        batch_object = client.batches.retrieve(f\"batch_{batch_id}\")\n",
    "        batch_status = batch_object.status\n",
    "\n",
    "        logging.info(f\"Batch {batch_id} status: {batch_status}\")\n",
    "\n",
    "        if batch_status in [\"completed\", \"failed\"]:\n",
    "            return batch_object\n",
    "        elif batch_status in [\"created\", \"in_progress\", \"finalizing\", \"validating\"]:\n",
    "            return None\n",
    "        else:\n",
    "            logging.warning(f\"Batch {batch_id} has unexpected status: {batch_status}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving batch {batch_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process completed batches\n",
    "def process_completed_batch(batch_id, batch_info, batch_processing_file):\n",
    "    error_occurred = False\n",
    "\n",
    "    if batch_info.output_file_id:\n",
    "        try:\n",
    "            results_response = client.files.content(batch_info.output_file_id)\n",
    "            completed_file_path = f\"{RESULTS_BATCHES_DIR}/{os.path.splitext(batch_processing_file)[0]}_results.jsonl\"\n",
    "\n",
    "            with open(\n",
    "                completed_file_path, \"wb\"\n",
    "            ) as result_file:  # Use \"wb\" for binary write\n",
    "                result_file.write(\n",
    "                    results_response.read()\n",
    "                )  # Read binary content and write\n",
    "\n",
    "            logging.info(f\"Saved results for batch {batch_id} to {completed_file_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing completed batch {batch_id}: {e}\")\n",
    "            error_occurred = True\n",
    "\n",
    "    if batch_info.error_file_id:\n",
    "        try:\n",
    "            error_response = client.files.content(batch_info.error_file_id)\n",
    "            failed_file_path = f\"{FAILED_BATCHES_DIR}/{os.path.splitext(batch_processing_file)[0]}_errors.jsonl\"\n",
    "\n",
    "            with open(\n",
    "                failed_file_path, \"wb\"\n",
    "            ) as error_file:  # Use \"wb\" for binary write\n",
    "                error_file.write(error_response.read())  # Read binary content and write\n",
    "\n",
    "            logging.info(f\"Saved errors for batch {batch_id} to {failed_file_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing failed {batch_id}: {e}\")\n",
    "            error_occurred = True\n",
    "\n",
    "    if not error_occurred:\n",
    "        # Move the processing file to processed directory\n",
    "        processed_file_path = f\"{PROCESSED_BATCHES_DIR}/{batch_processing_file}\"\n",
    "        move_file_to_directory(\n",
    "            f\"{PROCESSING_BATCHES_DIR}/{batch_processing_file}\", processed_file_path\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"Moved processing file for batch {batch_id} to processed directory\"\n",
    "        )\n",
    "\n",
    "# Function to get processing batches\n",
    "def get_processing_batches():\n",
    "    if not os.path.exists(PROCESSING_BATCHES_DIR):\n",
    "        logging.warning(f\"Processing directory {PROCESSING_BATCHES_DIR} does not exist.\")\n",
    "        return []\n",
    "\n",
    "    processing_file_paths = os.listdir(PROCESSING_BATCHES_DIR)\n",
    "    if not processing_file_paths:\n",
    "        logging.info(\"No batches are currently being processed.\")\n",
    "        return []\n",
    "\n",
    "    return processing_file_paths\n",
    "\n",
    "# Main function to check on batches being processed\n",
    "def check_batches_processing():\n",
    "    processing_file_paths = get_processing_batches()\n",
    "\n",
    "    if not processing_file_paths:\n",
    "        logging.info(\"No batches are currently being processed.\")\n",
    "        return\n",
    "\n",
    "    for batch_file in processing_file_paths:\n",
    "        try:\n",
    "            batch_id = batch_file.split('_id_')[1].replace('.jsonl', '')\n",
    "            batch_info = retrieve_batch_status(batch_id)\n",
    "\n",
    "            if batch_info:\n",
    "                process_completed_batch(\n",
    "                    batch_id,\n",
    "                    batch_info,\n",
    "                    batch_file\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing batch file {batch_file}: {e}\")\n",
    "\n",
    "# Call the batches processing check function\n",
    "check_batches_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec472771",
   "metadata": {},
   "source": [
    "### Save Tasks Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278c90c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 20:03:16,139 - WARNING - Some batches failed. Check the ../data/fakebr/batches/datasets_generation_jobs/failed/gpt-5-nano_2025-11-06_18-00-24 directory for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading results file: ../data/fakebr/batches/datasets_generation_jobs/results/gpt-5-nano_2025-11-06_18-00-24/batch_1_id_690d0ca441388190a5ccf76c01a7a0bc_results.jsonl\n",
      "Reading results file: ../data/fakebr/batches/datasets_generation_jobs/results/gpt-5-nano_2025-11-06_18-00-24/batch_2_id_690d0cc1bf748190b986c065565d686b_results.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Get all files from errors directory and log them\n",
    "error_files = os.listdir(FAILED_BATCHES_DIR)\n",
    "if error_files:\n",
    "    logging.warning(\n",
    "        f\"Some batches failed. Check the {FAILED_BATCHES_DIR} directory for details.\"\n",
    "    )\n",
    "else:\n",
    "    logging.info(f\"All batches in process {PROCESS_ID} completed successfully.\")\n",
    "\n",
    "# Load original dataset for reference\n",
    "original_dataset_df = pd.DataFrame()\n",
    "for file in ORIGINAL_DATASET_FILES:\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(file)\n",
    "        original_dataset_df = pd.concat([original_dataset_df, df], ignore_index=True)\n",
    "\n",
    "# Get all files in the results directory\n",
    "os.listdir(RESULTS_BATCHES_DIR)\n",
    "\n",
    "if not os.listdir(RESULTS_BATCHES_DIR):\n",
    "\t\tlogging.error(f\"No result files found in {RESULTS_BATCHES_DIR}.\")\n",
    "else:\n",
    "\t\t# Load all batches into a single DataFrame\n",
    "\t\tall_results_df = pd.DataFrame()\n",
    "\t\tfor result_file in os.listdir(RESULTS_BATCHES_DIR):\n",
    "\t\t\t\tif result_file.endswith(\".jsonl\"):\n",
    "\t\t\t\t\t\tresult_file_path = os.path.join(RESULTS_BATCHES_DIR, result_file)\n",
    "\n",
    "\t\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\t\t# Initialize a list to hold rows\n",
    "\t\t\t\t\t\t\t\trows = []\n",
    "\n",
    "\t\t\t\t\t\t\t\t# Read the JSONL file line by line\n",
    "\t\t\t\t\t\t\t\twith open(result_file_path, \"r\") as file:\n",
    "\t\t\t\t\t\t\t\t\t\tprint(f\"Reading results file: {result_file_path}\")\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\tfor line in file:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t# Parse each line as JSON\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tjson_data = json.loads(line)\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t# Extract relevant information\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tif \"extrnorm\" in json_data.get(\"custom_id\", \"\"):\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\ttask = \"claim_extraction_normalization\"\n",
    "\t\t\t\t\t\t\t\t\t\t\t\telif \"extr\" in json_data.get(\"custom_id\", \"\"):\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\ttask = \"claim_extraction\"\n",
    "\t\t\t\t\t\t\t\t\t\t\t\telif \"norm\" in json_data.get(\"custom_id\", \"\"):\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\ttask = \"claim_normalization\"\n",
    "\t\t\t\t\t\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tlogging.warning(f\"Unknown task for custom_id: {json_data.get('custom_id')}\")\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\ttask = \"unknown\"\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\t\t\toriginal_custom_id = (\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tjson_data.get(\"custom_id\")\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.replace(\"_extrnorm\", \"\")\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.replace(\"_extr\", \"\")\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.replace(\"_norm\", \"\")\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\toriginal_classificacao = (\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\toriginal_dataset_df[\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\toriginal_dataset_df[\"custom_id\"] == original_custom_id\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t][\"classificacao\"].values[0]\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tif not original_dataset_df[\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\toriginal_dataset_df[\"custom_id\"] == original_custom_id\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t].empty\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\telse None\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t)\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t# Create a row with relevant information\n",
    "\t\t\t\t\t\t\t\t\t\t\t\trow = {\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"custom_id\": original_custom_id,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"text\": json_data.get(\"response\", {})\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.get(\"body\", {})\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.get(\"choices\", [{}])[0]\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.get(\"message\", {})\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.get(\"content\", None),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"classificacao\": original_classificacao,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"source\": \"train\" if \"train\" in original_custom_id else \"test\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"task\": task,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t}\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t# Skip rows with empty content\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tif row[\"text\"] is not None and row[\"classificacao\"] is not None:\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t# Append row to rows list\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\trows.append(row)\n",
    "\n",
    "\t\t\t\t\t\t\t\t# Convert rows to DataFrame and concatenate to all_results_df\n",
    "\t\t\t\t\t\t\t\tbatch_df = pd.json_normalize(rows)\n",
    "\t\t\t\t\t\t\t\tall_results_df = pd.concat([all_results_df, batch_df], ignore_index=True)\n",
    "\n",
    "\t\t\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\t\t\t\tlogging.error(f\"Error reading results file {result_file_path}: {e}\")\n",
    "\n",
    "\t\t# Separate by different tasks (claim extraction and claim normalization)\n",
    "\t\textraction_df = all_results_df[all_results_df[\"task\"] == \"claim_extraction\"].copy()\n",
    "\t\tnormalization_df = all_results_df[\n",
    "\t\t\t\tall_results_df[\"task\"] == \"claim_normalization\"\n",
    "\t\t].copy()\n",
    "\t\textraction_normalization_df = all_results_df[\n",
    "\t\t\t\tall_results_df[\"task\"] == \"claim_extraction_normalization\"\n",
    "\t\t].copy()\n",
    "\t\textraction_df.drop(columns=[\"task\"], inplace=True)\n",
    "\t\tnormalization_df.drop(columns=[\"task\"], inplace=True)\n",
    "\t\textraction_normalization_df.drop(columns=[\"task\"], inplace=True)\n",
    "\n",
    "\t\t# Create output directories\n",
    "\t\textraction_output_path = f\"{DATASET_PATH}/claim_extraction\"\n",
    "\t\tnormalization_output_path = f\"{DATASET_PATH}/claim_normalization\"\n",
    "\t\textraction_normalization_output_path = f\"{DATASET_PATH}/claim_extraction_normalization\"\n",
    "\n",
    "\t\tos.makedirs(extraction_output_path, exist_ok=True)\n",
    "\t\tos.makedirs(normalization_output_path, exist_ok=True)\n",
    "\t\tos.makedirs(extraction_normalization_output_path, exist_ok=True)\n",
    "\n",
    "\t\t# Create a directory for each process\n",
    "\t\tos.makedirs(f\"{extraction_output_path}/{PROCESS_ID}\", exist_ok=True)\n",
    "\t\tos.makedirs(f\"{normalization_output_path}/{PROCESS_ID}\", exist_ok=True)\n",
    "\t\tos.makedirs(f\"{extraction_normalization_output_path}/{PROCESS_ID}\", exist_ok=True)\n",
    "\n",
    "\t\t# Split each DataFrame into train and test based on the source column\n",
    "\t\textraction_train_df = extraction_df[extraction_df[\"source\"] == \"train\"].drop(columns=[\"source\"])\n",
    "\t\textraction_test_df = extraction_df[extraction_df[\"source\"] == \"test\"].drop(columns=[\"source\"])\n",
    "\t\tnormalization_train_df = normalization_df[normalization_df[\"source\"] == \"train\"].drop(columns=[\"source\"])\n",
    "\t\tnormalization_test_df = normalization_df[normalization_df[\"source\"] == \"test\"].drop(columns=[\"source\"])\n",
    "\t\textraction_normalization_train_df = extraction_normalization_df[extraction_normalization_df[\"source\"] == \"train\"].drop(columns=[\"source\"])\n",
    "\t\textraction_normalization_test_df = extraction_normalization_df[extraction_normalization_df[\"source\"] == \"test\"].drop(columns=[\"source\"])\n",
    "\n",
    "\t\t# Save train and test CSVs\n",
    "\t\textraction_train_df.to_csv(\n",
    "\t\t\t\tf\"{extraction_output_path}/{PROCESS_ID}/train.csv\", index=False\n",
    "\t\t)\n",
    "\t\textraction_test_df.to_csv(\n",
    "\t\t\t\tf\"{extraction_output_path}/{PROCESS_ID}/test.csv\", index=False\n",
    "\t\t)\n",
    "\t\tnormalization_train_df.to_csv(\n",
    "\t\t\t\tf\"{normalization_output_path}/{PROCESS_ID}/train.csv\", index=False\n",
    "\t\t)\n",
    "\t\tnormalization_test_df.to_csv(\n",
    "\t\t\t\tf\"{normalization_output_path}/{PROCESS_ID}/test.csv\", index=False\n",
    "\t\t)\n",
    "\t\textraction_normalization_train_df.to_csv(\n",
    "\t\t\t\tf\"{extraction_normalization_output_path}/{PROCESS_ID}/train.csv\", index=False\n",
    "\t\t)\n",
    "\t\textraction_normalization_test_df.to_csv(\n",
    "\t\t\t\tf\"{extraction_normalization_output_path}/{PROCESS_ID}/test.csv\", index=False\n",
    "\t\t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
