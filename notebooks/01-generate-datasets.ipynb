{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fd3d139",
   "metadata": {},
   "source": [
    "# 01 - Generate Datasets\n",
    "\n",
    "This notebook is responsible for generating two new datasets from an original dataset. The process consists on passing the original dataset through two tasks: claim extraction and claim normalization. The outputs of each task is the saved to be used in the next steps of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e669923",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e9c5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Native\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Third-party\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Local\n",
    "from shared.prompts import (\n",
    "    CLAIM_EXTRACTION_SYSTEM_MESSAGE,\n",
    "    CLAIM_NORMALIZATION_SYSTEM_MESSAGE,\n",
    ")\n",
    "from shared.utils import move_file_to_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c832ba5",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "751b327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ce408d",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATASET = \"faketweetbr\" # Dataset that will be used to generate new datasets after claim extraction and normalization tasks.\n",
    "MODEL = \"gpt-5\" # Model that will be used to generate the datasets.\n",
    "DATASET_IMPORTANT_COLUMNS = [\"text\", \"classificacao\"] # All important columns in the dataset except index/id cloumns (custom id will be added automatically)\n",
    "PROCESS_TIME = None  # This variable is used to resume the processing of a previous execution. If None, current timestamp will be used.\n",
    "PROCESS_ID = f\"{MODEL}_{PROCESS_TIME if PROCESS_TIME else pd.Timestamp.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "# Directory paths\n",
    "DATASET_PATH = f\"../data/{DATASET}\"\n",
    "ORIGINAL_DATASET_PATH = DATASET_PATH + \"/original\"\n",
    "UNPROCESSED_BATCHES_DIR = f\"{DATASET_PATH}/batches/datasets_generation_jobs/unprocessed/{PROCESS_ID}\"\n",
    "UPLOADED_BATCHES_DIR = f\"{DATASET_PATH}/batches/datasets_generation_jobs/uploaded/{PROCESS_ID}\"\n",
    "PROCESSING_BATCHES_DIR = f\"{DATASET_PATH}/batches/datasets_generation_jobs/processing/{PROCESS_ID}\"\n",
    "PROCESSED_BATCHES_DIR = f\"{DATASET_PATH}/batches/datasets_generation_jobs/processed/{PROCESS_ID}\"\n",
    "RESULTS_BATCHES_DIR = f\"{DATASET_PATH}/batches/datasets_generation_jobs/results/{PROCESS_ID}\"\n",
    "FAILED_BATCHES_DIR = f\"{DATASET_PATH}/batches/datasets_generation_jobs/failed/{PROCESS_ID}\"\n",
    "\n",
    "# OpenAI batch processing parameters\n",
    "COMPLETION_ENDPOINT = \"/v1/chat/completions\"\n",
    "MAX_COMPLETION_TOKENS = None\n",
    "TEMPERATURE = 1 # Obs: gpt-5-nano does not support temperature=0\n",
    "VERBOSITY = \"low\" # Options: \"low\", \"medium\", \"high\"\n",
    "REASONING_EFFORT = \"high\" # Options: \"low\", \"medium\", \"high\"\n",
    "ROWS_PER_BATCH = 5000 # Number of rows to process in each batch (will generate double the amount of calls due to executing two tasks: claim extraction and claim normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bcdc50",
   "metadata": {},
   "source": [
    "### Create Auxiliary Directories for Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55835f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure all necessary directories exist\n",
    "os.makedirs(UNPROCESSED_BATCHES_DIR, exist_ok=True)\n",
    "os.makedirs(UPLOADED_BATCHES_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSING_BATCHES_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_BATCHES_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_BATCHES_DIR, exist_ok=True)\n",
    "os.makedirs(FAILED_BATCHES_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b3cb4e",
   "metadata": {},
   "source": [
    "### Load Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d745291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 10:24:48,336 - INFO - Total records: 299\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "classificacao",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "custom_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "0647573b-f078-453f-9559-458998d3d709",
       "rows": [
        [
         "0",
         "Marielle >BANDIDOS Narco-traficantes-Milícias pisou na bola PCC >Mandante do crime #Brazão do PT #OrganizaçãoCriminosa Assassinada por THIAGO- MACACO Midia para de acusar quem destruiu a sonhada #GRANDEPATRIA @ECantanhede @CarvalhosaMo @CarlosBolsonaro @RenovMidia @roxmo @SITEFCS pic.twitter.com/8HTb0VVmj8",
         "fake",
         "faketweetbr_train_0",
         "train"
        ],
        [
         "1",
         "Bem, as últimas noticias a respeito disso que o verdadeiro assassino de Marielle não é aqueles acusados senão um elemento de codinome  Macaco \". Ou seja, não colou a relação Bolsonaro x Marielle . Reveja suas fontes.\"",
         "fake",
         "faketweetbr_train_1",
         "train"
        ],
        [
         "2",
         "@jornalnacional convivi com notícias da Marielle durante 3 meses quase todos os dias. Porque pararam as reportagens? Só porque se descobriu que seu matador foi Thiago macaco ? Negro favelado e ligado ao tráfico? Bonemer vc é o supra sumo do ridículo .",
         "fake",
         "faketweetbr_train_2",
         "train"
        ],
        [
         "3",
         "O Cesari Battisti confessou seus crimes, a esquerda calou; o assassino de MARIELLE foi descoberto, a esquerda se calou e agora o Luladrão confessou, a esquerda meteu a língua onde o macaco meu o caju. Demagogos, hipocritas e fariseus",
         "fake",
         "faketweetbr_train_3",
         "train"
        ],
        [
         "4",
         "[Agência Lupa] Verificamos: É falso que Thiago Macaco foi identificado como assassino de Marielle https:// piaui.folha.uol.com.br/lupa/2019/04/0 1/verificamos-marielle-preso-thiago/ …",
         "true",
         "faketweetbr_train_4",
         "train"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>classificacao</th>\n",
       "      <th>custom_id</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marielle &gt;BANDIDOS Narco-traficantes-Milícias ...</td>\n",
       "      <td>fake</td>\n",
       "      <td>faketweetbr_train_0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bem, as últimas noticias a respeito disso que ...</td>\n",
       "      <td>fake</td>\n",
       "      <td>faketweetbr_train_1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@jornalnacional convivi com notícias da Mariel...</td>\n",
       "      <td>fake</td>\n",
       "      <td>faketweetbr_train_2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O Cesari Battisti confessou seus crimes, a esq...</td>\n",
       "      <td>fake</td>\n",
       "      <td>faketweetbr_train_3</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Agência Lupa] Verificamos: É falso que Thiago...</td>\n",
       "      <td>true</td>\n",
       "      <td>faketweetbr_train_4</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text classificacao  \\\n",
       "0  Marielle >BANDIDOS Narco-traficantes-Milícias ...          fake   \n",
       "1  Bem, as últimas noticias a respeito disso que ...          fake   \n",
       "2  @jornalnacional convivi com notícias da Mariel...          fake   \n",
       "3  O Cesari Battisti confessou seus crimes, a esq...          fake   \n",
       "4  [Agência Lupa] Verificamos: É falso que Thiago...          true   \n",
       "\n",
       "             custom_id source  \n",
       "0  faketweetbr_train_0  train  \n",
       "1  faketweetbr_train_1  train  \n",
       "2  faketweetbr_train_2  train  \n",
       "3  faketweetbr_train_3  train  \n",
       "4  faketweetbr_train_4  train  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty DataFrame to hold the concatenated dataset\n",
    "dataset_df = pd.DataFrame()\n",
    "\n",
    "# Read all CSV files in the dataset directory and concatenate them into a single DataFrame\n",
    "for file in os.listdir(ORIGINAL_DATASET_PATH):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(ORIGINAL_DATASET_PATH, file))\n",
    "        \n",
    "\t\t\t\t# Row source\n",
    "        row_source = file.replace(\".csv\", \"\")\n",
    "\n",
    "        # If the custom_id column does not exist, create it and save the updated dataframe back to the csv file\n",
    "        if \"custom_id\" not in df.columns:\n",
    "            df[\"custom_id\"] = (\n",
    "                DATASET + \"_\" + row_source + \"_\" + df.index.astype(str)\n",
    "            )  # Add a custom_id column to keep track of original row positions\n",
    "\n",
    "            # Put the custom_id column at the front\n",
    "            cols = df.columns.tolist()\n",
    "            cols = [cols[-1]] + cols[:-1]  # Move custom_id to the front\n",
    "            df = df[cols]\n",
    "\n",
    "            # Save the updated dataframe back to the csv file\n",
    "            df.to_csv(os.path.join(ORIGINAL_DATASET_PATH, file), index=False)\n",
    "\n",
    "        # Add a source column to identify the origin of each row\n",
    "        df[\"source\"] = row_source\n",
    "\n",
    "        # Concatenate the current dataframe to the main dataset dataframe\n",
    "        dataset_df = pd.concat([dataset_df, df], ignore_index=True)\n",
    "\n",
    "# Keep only relevant columns\n",
    "dataset_df = dataset_df[DATASET_IMPORTANT_COLUMNS + [\"custom_id\", \"source\"]] # Add custom id and source columns to keep track of original rows\n",
    "\n",
    "# Display the first few rows of the concatenated dataset\n",
    "logging.info(f\"Total records: {len(dataset_df)}\")\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4b5af4",
   "metadata": {},
   "source": [
    "### Create Claim Extraction and Normalization Jobs Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1590c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 10:24:48,386 - INFO - Saved batch to ../data/faketweetbr/batches/unprocessed/gpt-5-nano_2025-10-13_10-24-48/batch_1.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 10:24:49,822 - INFO - HTTP Request: POST https://api.openai.com/v1/files \"HTTP/1.1 200 OK\"\n",
      "2025-10-13 10:24:49,824 - INFO - Uploaded batch to OpenAI successfully! File ID: file-HBBoAVTUe3UVnGmE3DWnzW\n",
      "2025-10-13 10:24:49,825 - INFO - Moved file to ../data/faketweetbr/batches/uploaded/gpt-5-nano_2025-10-13_10-24-48/batch_1_file-id_file-HBBoAVTUe3UVnGmE3DWnzW.jsonl\n",
      "2025-10-13 10:24:51,941 - INFO - HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "2025-10-13 10:24:51,942 - INFO - Created batch successfully! Batch ID: batch_68ecfda36f4c819083c7ee9ce6cb7b7b\n",
      "2025-10-13 10:24:51,943 - INFO - Moved file to ../data/faketweetbr/batches/processing/gpt-5-nano_2025-10-13_10-24-48/batch_1_id_68ecfda36f4c819083c7ee9ce6cb7b7b.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Function to generate JSONL rows for a batch\n",
    "def generate_batch_jsonl_rows(batch_df):\n",
    "    batch_jsonl_rows = []\n",
    "\n",
    "    for _, row in batch_df.iterrows():\n",
    "        claim_extraction_row_dict = {\n",
    "            \"custom_id\": f\"{row['custom_id']}_extr\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": COMPLETION_ENDPOINT,\n",
    "            \"body\": {\n",
    "                \"model\": MODEL,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"developer\", \"content\": CLAIM_EXTRACTION_SYSTEM_MESSAGE},\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Postagem: {row['text']}\\nDeclaração extraída:\",\n",
    "                    },\n",
    "                ],\n",
    "                \"max_completion_tokens\": MAX_COMPLETION_TOKENS,\n",
    "                \"metadata\": {\n",
    "                    \"custom_id\": row[\"custom_id\"],\n",
    "                    \"source\": row[\"source\"],\n",
    "                    \"classificacao\": row[\"classificacao\"],\n",
    "                    \"task\": \"claim_extraction\",\n",
    "                },\n",
    "                \"verbosity\": VERBOSITY,\n",
    "                \"reasoning_effort\": REASONING_EFFORT,\n",
    "                \"temperature\": TEMPERATURE,\n",
    "                \"store\": True,\n",
    "            },\n",
    "        }\n",
    "        claim_normalization_row_dict = {\n",
    "            \"custom_id\": f\"{row['custom_id']}_norm\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": COMPLETION_ENDPOINT,\n",
    "            \"body\": {\n",
    "                \"model\": MODEL,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"developer\",\n",
    "                        \"content\": CLAIM_NORMALIZATION_SYSTEM_MESSAGE,\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Postagem: {row['text']}\\nDeclaração normalizada:\",\n",
    "                    },\n",
    "                ],\n",
    "                \"max_completion_tokens\": MAX_COMPLETION_TOKENS,\n",
    "                \"metadata\": {\n",
    "                    \"custom_id\": row[\"custom_id\"],\n",
    "                    \"source\": row[\"source\"],\n",
    "                    \"classificacao\": row[\"classificacao\"],\n",
    "                    \"task\": \"claim_normalization\",\n",
    "                },\n",
    "                \"verbosity\": VERBOSITY,\n",
    "                \"reasoning_effort\": REASONING_EFFORT,\n",
    "                \"temperature\": TEMPERATURE,\n",
    "                \"store\": True,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        batch_jsonl_rows.append(claim_extraction_row_dict)\n",
    "        batch_jsonl_rows.append(claim_normalization_row_dict)\n",
    "\n",
    "    return batch_jsonl_rows\n",
    "\n",
    "\n",
    "# Function to save JSONL file\n",
    "def save_batch_jsonl_file(batch_jsonl_rows, batch_file_path):\n",
    "    try:\n",
    "        with open(batch_file_path, \"w\") as jsonl_file:\n",
    "            for row in batch_jsonl_rows:\n",
    "                jsonl_file.write(\n",
    "                    json.dumps(row) + \"\\n\"\n",
    "                )  # Use json.dumps to format with double quotes\n",
    "        logging.info(f\"Saved batch to {batch_file_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving batch to {batch_file_path}: {e}\")\n",
    "\n",
    "\n",
    "# Function to upload file to OpenAI\n",
    "def upload_batch_file_to_openai(batch_file_path):\n",
    "    try:\n",
    "        batch_uploaded_file = client.files.create(\n",
    "            file=open(batch_file_path, \"rb\"), purpose=\"batch\"\n",
    "        )\n",
    "        logging.info(f\"Uploaded batch to OpenAI successfully! File ID: {batch_uploaded_file.id}\")\n",
    "        return batch_uploaded_file.id\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error uploading batch to OpenAI: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to create a batch in OpenAI\n",
    "def create_openai_batch(batch_input_file_id):\n",
    "    try:\n",
    "        batch_info = client.batches.create(\n",
    "            input_file_id=batch_input_file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\"description\": f\"Batch created from file ID {batch_input_file_id}\"},\n",
    "        )\n",
    "        logging.info(f\"Created batch successfully! Batch ID: {batch_info.id}\")\n",
    "        return batch_info.id\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating batch: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main batch processing loop\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "current_batch = 1\n",
    "\n",
    "for i in range(0, len(dataset_df), ROWS_PER_BATCH):\n",
    "    # Generate JSONL rows for the current batch\n",
    "    batch_df = dataset_df.iloc[i:i + ROWS_PER_BATCH]\n",
    "    batch_jsonl_rows = generate_batch_jsonl_rows(batch_df)\n",
    "\n",
    "    # Save the batch to a JSONL file\n",
    "    batch_file_name = f\"batch_{current_batch}_{PROCESS_ID}.jsonl\"\n",
    "    batch_file_path = f\"{UNPROCESSED_BATCHES_DIR}/{batch_file_name}\"\n",
    "    save_batch_jsonl_file(batch_jsonl_rows, batch_file_path)\n",
    "\n",
    "    # Upload the batch file to OpenAI\n",
    "    batch_input_file_id = upload_batch_file_to_openai(batch_file_path)\n",
    "\n",
    "    if batch_input_file_id:\n",
    "        # Move the batch file to the uploaded directory\n",
    "        uploaded_file_path = f\"{UPLOADED_BATCHES_DIR}/batch_{current_batch}_file-id_{batch_input_file_id}.jsonl\"\n",
    "        move_file_to_directory(batch_file_path, uploaded_file_path)\n",
    "\n",
    "        # Create the batch in OpenAI\n",
    "        batch_id = create_openai_batch(batch_input_file_id)\n",
    "\n",
    "        if batch_id:\n",
    "            # Move the batch file to the processing directory\n",
    "            processing_file_path = f\"{PROCESSING_BATCHES_DIR}/batch_{current_batch}_id_{batch_id.replace(\"batch_\", \"\")}.jsonl\"\n",
    "            move_file_to_directory(uploaded_file_path, processing_file_path)\n",
    "\n",
    "    # Increment the batch counter\n",
    "    current_batch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28dfbbe",
   "metadata": {},
   "source": [
    "### Check Batches Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bf3068d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 16:15:51,013 - INFO - HTTP Request: GET https://api.openai.com/v1/batches/batch_68ecfd39f15881908a0896906a0c528d \"HTTP/1.1 200 OK\"\n",
      "2025-10-13 16:15:51,014 - INFO - Batch 68ecfd39f15881908a0896906a0c528d status: completed\n",
      "2025-10-13 16:15:51,996 - INFO - HTTP Request: GET https://api.openai.com/v1/files/file-KtAMrm4X7Wev3n1T6xh268/content \"HTTP/1.1 200 OK\"\n",
      "2025-10-13 16:15:52,694 - INFO - Saved results for batch 68ecfd39f15881908a0896906a0c528d to ../data/faketweetbr/batches/results/gpt-5_2025-10-13_10-23-02/batch_1_id_68ecfd39f15881908a0896906a0c528d_results.jsonl\n",
      "2025-10-13 16:15:52,695 - INFO - Moved file to ../data/faketweetbr/batches/processed/gpt-5_2025-10-13_10-23-02/batch_1_id_68ecfd39f15881908a0896906a0c528d.jsonl\n",
      "2025-10-13 16:15:52,695 - INFO - Moved processing file for batch 68ecfd39f15881908a0896906a0c528d to processed directory\n"
     ]
    }
   ],
   "source": [
    "# Function to retrieve batch status from OpenAI\n",
    "def retrieve_batch_status(batch_id):\n",
    "    try:\n",
    "        batch_object = client.batches.retrieve(f\"batch_{batch_id}\")\n",
    "        batch_status = batch_object.status\n",
    "\n",
    "        logging.info(f\"Batch {batch_id} status: {batch_status}\")\n",
    "\n",
    "        if batch_status in [\"completed\", \"failed\"]:\n",
    "            return batch_object\n",
    "        elif batch_status in [\"created\", \"in_progress\", \"finalizing\", \"validating\"]:\n",
    "            return None\n",
    "        else:\n",
    "            logging.warning(f\"Batch {batch_id} has unexpected status: {batch_status}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving batch {batch_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process completed batches\n",
    "def process_completed_batch(batch_id, batch_info, batch_processing_file):\n",
    "    error_occurred = False\n",
    "\n",
    "    if batch_info.output_file_id:\n",
    "        try:\n",
    "            results_response = client.files.content(batch_info.output_file_id)\n",
    "            completed_file_path = f\"{RESULTS_BATCHES_DIR}/{os.path.splitext(batch_processing_file)[0]}_results.jsonl\"\n",
    "\n",
    "            with open(\n",
    "                completed_file_path, \"wb\"\n",
    "            ) as result_file:  # Use \"wb\" for binary write\n",
    "                result_file.write(\n",
    "                    results_response.read()\n",
    "                )  # Read binary content and write\n",
    "\n",
    "            logging.info(f\"Saved results for batch {batch_id} to {completed_file_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing completed batch {batch_id}: {e}\")\n",
    "            error_occurred = True\n",
    "\n",
    "    if batch_info.error_file_id:\n",
    "        try:\n",
    "            error_response = client.files.content(batch_info.error_file_id)\n",
    "            failed_file_path = f\"{FAILED_BATCHES_DIR}/{os.path.splitext(batch_processing_file)[0]}_errors.jsonl\"\n",
    "\n",
    "            with open(\n",
    "                failed_file_path, \"wb\"\n",
    "            ) as error_file:  # Use \"wb\" for binary write\n",
    "                error_file.write(error_response.read())  # Read binary content and write\n",
    "\n",
    "            logging.info(f\"Saved errors for batch {batch_id} to {failed_file_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing failed {batch_id}: {e}\")\n",
    "            error_occurred = True\n",
    "\n",
    "    if not error_occurred:\n",
    "        # Move the processing file to processed directory\n",
    "        processed_file_path = f\"{PROCESSED_BATCHES_DIR}/{batch_processing_file}\"\n",
    "        move_file_to_directory(\n",
    "            f\"{PROCESSING_BATCHES_DIR}/{batch_processing_file}\", processed_file_path\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"Moved processing file for batch {batch_id} to processed directory\"\n",
    "        )\n",
    "\n",
    "# Function to get processing batches\n",
    "def get_processing_batches():\n",
    "    if not os.path.exists(PROCESSING_BATCHES_DIR):\n",
    "        logging.warning(f\"Processing directory {PROCESSING_BATCHES_DIR} does not exist.\")\n",
    "        return []\n",
    "\n",
    "    processing_file_paths = os.listdir(PROCESSING_BATCHES_DIR)\n",
    "    if not processing_file_paths:\n",
    "        logging.info(\"No batches are currently being processed.\")\n",
    "        return []\n",
    "\n",
    "    return processing_file_paths\n",
    "\n",
    "# Main function to check on batches being processed\n",
    "def check_batches_processing():\n",
    "    processing_file_paths = get_processing_batches()\n",
    "\n",
    "    if not processing_file_paths:\n",
    "        logging.info(\"No batches are currently being processed.\")\n",
    "        return\n",
    "\n",
    "    for batch_file in processing_file_paths:\n",
    "        try:\n",
    "            batch_id = batch_file.split('_id_')[1].replace('.jsonl', '')\n",
    "            batch_info = retrieve_batch_status(batch_id)\n",
    "\n",
    "            if batch_info:\n",
    "                process_completed_batch(\n",
    "                    batch_id,\n",
    "                    batch_info,\n",
    "                    batch_file\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing batch file {batch_file}: {e}\")\n",
    "\n",
    "# Call the batches processing check function\n",
    "check_batches_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec472771",
   "metadata": {},
   "source": [
    "### Save Claim Extraction and Normalization Tasks Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "278c90c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 16:16:02,693 - INFO - All batches in process gpt-5_2025-10-13_10-23-02 completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading results file: ../data/faketweetbr/batches/results/gpt-5_2025-10-13_10-23-02/batch_1_id_68ecfd39f15881908a0896906a0c528d_results.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Get all files from errors directory and log them\n",
    "error_files = os.listdir(FAILED_BATCHES_DIR)\n",
    "if error_files:\n",
    "    logging.warning(\n",
    "        f\"Some batches failed. Check the {FAILED_BATCHES_DIR} directory for details.\"\n",
    "    )\n",
    "else:\n",
    "    logging.info(f\"All batches in process {PROCESS_ID} completed successfully.\")\n",
    "\n",
    "# Load original dataset for reference\n",
    "original_dataset_df = pd.DataFrame()\n",
    "for file in os.listdir(ORIGINAL_DATASET_PATH):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(ORIGINAL_DATASET_PATH, file))\n",
    "        original_dataset_df = pd.concat([original_dataset_df, df], ignore_index=True)\n",
    "\n",
    "# Get all files in the results directory\n",
    "os.listdir(RESULTS_BATCHES_DIR)\n",
    "\n",
    "# Load all batches into a single DataFrame\n",
    "all_results_df = pd.DataFrame()\n",
    "for result_file in os.listdir(RESULTS_BATCHES_DIR):\n",
    "    if result_file.endswith(\".jsonl\"):\n",
    "        result_file_path = os.path.join(RESULTS_BATCHES_DIR, result_file)\n",
    "\n",
    "        try:\n",
    "            # Initialize a list to hold rows\n",
    "            rows = []\n",
    "\n",
    "            # Read the JSONL file line by line\n",
    "            with open(result_file_path, \"r\") as file:\n",
    "                print(f\"Reading results file: {result_file_path}\")\n",
    "\n",
    "                for line in file:\n",
    "                    # Parse each line as JSON\n",
    "                    json_data = json.loads(line)\n",
    "\n",
    "                    # Extract relevant information\n",
    "                    task = (\n",
    "                        \"claim_extraction\"\n",
    "                        if \"extr\" in json_data.get(\"custom_id\", \"\")\n",
    "                        else \"claim_normalization\"\n",
    "                    )\n",
    "                    original_custom_id = (\n",
    "                        json_data.get(\"custom_id\")\n",
    "                        .replace(\"_extr\", \"\")\n",
    "                        .replace(\"_norm\", \"\")\n",
    "                    )\n",
    "                    original_classificacao = (\n",
    "                        original_dataset_df[\n",
    "                            original_dataset_df[\"custom_id\"] == original_custom_id\n",
    "                        ][\"classificacao\"].values[0]\n",
    "                        if not original_dataset_df[\n",
    "                            original_dataset_df[\"custom_id\"] == original_custom_id\n",
    "                        ].empty\n",
    "                        else None\n",
    "                    )\n",
    "\n",
    "                    # Create a row with relevant information\n",
    "                    row = {\n",
    "                        \"custom_id\": original_custom_id,\n",
    "                        \"text\": json_data.get(\"response\", {})\n",
    "                        .get(\"body\", {})\n",
    "                        .get(\"choices\", [{}])[0]\n",
    "                        .get(\"message\", {})\n",
    "                        .get(\"content\", None),\n",
    "                        \"classificacao\": original_classificacao,\n",
    "                        \"source\": \"train\" if \"train\" in original_custom_id else \"test\",\n",
    "                        \"task\": task,\n",
    "                    }\n",
    "\n",
    "                    # Skip rows with empty content\n",
    "                    if row[\"text\"] is not None and row[\"classificacao\"] is not None:\n",
    "\n",
    "                        # Append row to rows list\n",
    "                        rows.append(row)\n",
    "\n",
    "            # Convert rows to DataFrame and concatenate to all_results_df\n",
    "            batch_df = pd.json_normalize(rows)\n",
    "            all_results_df = pd.concat([all_results_df, batch_df], ignore_index=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error reading results file {result_file_path}: {e}\")\n",
    "\n",
    "# Separate by different tasks (claim extraction and claim normalization)\n",
    "extraction_df = all_results_df[all_results_df[\"task\"] == \"claim_extraction\"].copy()\n",
    "normalization_df = all_results_df[\n",
    "    all_results_df[\"task\"] == \"claim_normalization\"\n",
    "].copy()\n",
    "extraction_df.drop(columns=[\"task\"], inplace=True)\n",
    "normalization_df.drop(columns=[\"task\"], inplace=True)\n",
    "\n",
    "# Save each DataFrame to CSV\n",
    "extraction_output_path = f\"{DATASET_PATH}/claim_extraction\"\n",
    "normalization_output_path = f\"{DATASET_PATH}/claim_normalization\"\n",
    "os.makedirs(extraction_output_path, exist_ok=True)\n",
    "os.makedirs(normalization_output_path, exist_ok=True)\n",
    "extraction_df.to_csv(\n",
    "    f\"{extraction_output_path}/claim_extraction_{PROCESS_ID}.csv\", index=False\n",
    ")\n",
    "normalization_df.to_csv(\n",
    "    f\"{normalization_output_path}/claim_normalization_{PROCESS_ID}.csv\", index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
