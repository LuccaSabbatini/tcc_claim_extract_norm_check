{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e669923",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Native\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Third-party\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Local\n",
    "from shared.prompts import (\n",
    "    CLAIM_EXTRACTION_SYSTEM_MESSAGE,\n",
    "    CLAIM_NORMALIZATION_SYSTEM_MESSAGE,\n",
    ")\n",
    "from shared.utils import move_file_to_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c832ba5",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "751b327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ce408d",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3297a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory paths\n",
    "DATASET_PATH = \"../data/faketweetbr\"\n",
    "UNPROCESSED_BATCHES_DIR = f\"{DATASET_PATH}/batches/unprocessed\"\n",
    "UPLOADED_BATCHES_DIR = f\"{DATASET_PATH}/batches/uploaded\"\n",
    "PROCESSING_BATCHES_DIR = f\"{DATASET_PATH}/batches/processing\"\n",
    "PROCESSED_BATCHES_DIR = f\"{DATASET_PATH}/batches/processed\"\n",
    "RESULTS_BATCHES_DIR = f\"{DATASET_PATH}/batches/results\"\n",
    "FAILED_BATCHES_DIR = f\"{DATASET_PATH}/batches/failed\"\n",
    "\n",
    "# OpenAI batch processing parameters\n",
    "COMPLETION_ENDPOINT = \"/v1/chat/completions\"\n",
    "MODEL = \"gpt-5-nano\"\n",
    "MAX_COMPLETION_TOKENS = 500\n",
    "TEMPERATURE = 1 # Obs: gpt-5-nano does not support temperature=0\n",
    "VERBOSITY = \"low\" # Options: \"low\", \"medium\", \"high\"\n",
    "REASONING_EFFORT = \"high\" # Options: \"low\", \"medium\", \"high\"\n",
    "ROWS_PER_BATCH = 5000 # Number of rows to process in each batch (will generate double the amount of calls due to executing two tasks: claim extraction and claim normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b3cb4e",
   "metadata": {},
   "source": [
    "### Load Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d745291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 11:54:50,631 - INFO - Total records: 279\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "subject",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "classificacao",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "110b5858-938f-4362-9725-f277891a71b3",
       "rows": [
        [
         "0",
         "1.124513050218893e+18",
         "macaco marielle",
         "Marielle >BANDIDOS Narco-traficantes-Milícias pisou na bola PCC >Mandante do crime #Brazão do PT #OrganizaçãoCriminosa Assassinada por THIAGO- MACACO Midia para de acusar quem destruiu a sonhada #GRANDEPATRIA @ECantanhede @CarvalhosaMo @CarlosBolsonaro @RenovMidia @roxmo @SITEFCS pic.twitter.com/8HTb0VVmj8",
         "fake",
         "train"
        ],
        [
         "1",
         "1.1240493710064764e+18",
         "macaco marielle",
         "Bem, as últimas noticias a respeito disso que o verdadeiro assassino de Marielle não é aqueles acusados senão um elemento de codinome  Macaco \". Ou seja, não colou a relação Bolsonaro x Marielle . Reveja suas fontes.\"",
         "fake",
         "train"
        ],
        [
         "2",
         "1.119295029204398e+18",
         "macaco marielle",
         "@jornalnacional convivi com notícias da Marielle durante 3 meses quase todos os dias. Porque pararam as reportagens? Só porque se descobriu que seu matador foi Thiago macaco ? Negro favelado e ligado ao tráfico? Bonemer vc é o supra sumo do ridículo .",
         "fake",
         "train"
        ],
        [
         "3",
         "1.1145825456252273e+18",
         "macaco marielle",
         "O Cesari Battisti confessou seus crimes, a esquerda calou; o assassino de MARIELLE foi descoberto, a esquerda se calou e agora o Luladrão confessou, a esquerda meteu a língua onde o macaco meu o caju. Demagogos, hipocritas e fariseus",
         "fake",
         "train"
        ],
        [
         "4",
         "1.1132459115918624e+18",
         "macaco marielle",
         "[Agência Lupa] Verificamos: É falso que Thiago Macaco foi identificado como assassino de Marielle https:// piaui.folha.uol.com.br/lupa/2019/04/0 1/verificamos-marielle-preso-thiago/ …",
         "true",
         "train"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subject</th>\n",
       "      <th>text</th>\n",
       "      <th>classificacao</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.124513e+18</td>\n",
       "      <td>macaco marielle</td>\n",
       "      <td>Marielle &gt;BANDIDOS Narco-traficantes-Milícias ...</td>\n",
       "      <td>fake</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.124049e+18</td>\n",
       "      <td>macaco marielle</td>\n",
       "      <td>Bem, as últimas noticias a respeito disso que ...</td>\n",
       "      <td>fake</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.119295e+18</td>\n",
       "      <td>macaco marielle</td>\n",
       "      <td>@jornalnacional convivi com notícias da Mariel...</td>\n",
       "      <td>fake</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.114583e+18</td>\n",
       "      <td>macaco marielle</td>\n",
       "      <td>O Cesari Battisti confessou seus crimes, a esq...</td>\n",
       "      <td>fake</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.113246e+18</td>\n",
       "      <td>macaco marielle</td>\n",
       "      <td>[Agência Lupa] Verificamos: É falso que Thiago...</td>\n",
       "      <td>true</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id          subject  \\\n",
       "0  1.124513e+18  macaco marielle   \n",
       "1  1.124049e+18  macaco marielle   \n",
       "2  1.119295e+18  macaco marielle   \n",
       "3  1.114583e+18  macaco marielle   \n",
       "4  1.113246e+18  macaco marielle   \n",
       "\n",
       "                                                text classificacao source  \n",
       "0  Marielle >BANDIDOS Narco-traficantes-Milícias ...          fake  train  \n",
       "1  Bem, as últimas noticias a respeito disso que ...          fake  train  \n",
       "2  @jornalnacional convivi com notícias da Mariel...          fake  train  \n",
       "3  O Cesari Battisti confessou seus crimes, a esq...          fake  train  \n",
       "4  [Agência Lupa] Verificamos: É falso que Thiago...          true  train  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty DataFrame to hold the concatenated dataset\n",
    "original_dataset_path = DATASET_PATH + \"/original\"\n",
    "dataset_df = pd.DataFrame()\n",
    "\n",
    "# Read all CSV files in the dataset directory and concatenate them into a single DataFrame\n",
    "for file in os.listdir(original_dataset_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(original_dataset_path, file))\n",
    "        df[\"source\"] = file.replace(\".csv\", \"\") # Add a source column to identify the origin of each row\n",
    "        dataset_df = pd.concat([dataset_df, df], ignore_index=True)\n",
    "\n",
    "# Keep only relevant columns\n",
    "dataset_df = dataset_df[[\"id\", \"subject\", \"text\", \"classificacao\", \"source\"]]\n",
    "\n",
    "# Display the first few rows of the concatenated dataset\n",
    "logging.info(f\"Total records: {len(dataset_df)}\")\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4b5af4",
   "metadata": {},
   "source": [
    "### Create Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b1590c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 11:54:50,727 - INFO - Saved batch to ../data/faketweetbr/batches/unprocessed/batch_0_20250926_115450.jsonl\n",
      "2025-09-26 11:54:54,108 - INFO - HTTP Request: POST https://api.openai.com/v1/files \"HTTP/1.1 200 OK\"\n",
      "2025-09-26 11:54:54,110 - INFO - Uploaded batch to OpenAI successfully! File ID: file-GFkYDrePecZtf3pmSgpnfS\n",
      "2025-09-26 11:54:54,111 - INFO - Moved file to ../data/faketweetbr/batches/uploaded/batch-file-id_file-GFkYDrePecZtf3pmSgpnfS_batch_0_20250926_115450.jsonl\n",
      "2025-09-26 11:54:54,951 - INFO - HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "2025-09-26 11:54:54,957 - INFO - Created batch successfully! Batch ID: batch_68d6a93fb65881909cdd3c6a4975073f\n",
      "2025-09-26 11:54:54,958 - INFO - Moved file to ../data/faketweetbr/batches/processing/batch-id_batch_68d6a93fb65881909cdd3c6a4975073f_batch_0_20250926_115450.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Function to generate JSONL rows for a batch\n",
    "def generate_batch_jsonl_rows(batch_df):\n",
    "    batch_jsonl_rows = []\n",
    "\n",
    "    for _, row in batch_df.iterrows():\n",
    "        claim_extraction_row_dict = {\n",
    "            \"custom_id\": f\"{row['id']}_extr\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": COMPLETION_ENDPOINT,\n",
    "            \"body\": {\n",
    "                \"model\": MODEL,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"developer\", \"content\": CLAIM_EXTRACTION_SYSTEM_MESSAGE},\n",
    "                    {\"role\": \"user\", \"content\": f\"Postagem: {row['text']}\\nDeclaração extraída:\"}\n",
    "                ],\n",
    "                \"max_completion_tokens\": MAX_COMPLETION_TOKENS,\n",
    "                \"metadata\": {\n",
    "                    \"source\": row[\"source\"],\n",
    "                    \"subject\": row[\"subject\"],\n",
    "                    \"classificacao\": row[\"classificacao\"],\n",
    "                    \"task\": \"claim_extraction\"\n",
    "                },\n",
    "                \"verbosity\": VERBOSITY,\n",
    "                \"reasoning_effort\": REASONING_EFFORT,\n",
    "                \"temperature\": TEMPERATURE\n",
    "            }\n",
    "        }\n",
    "        claim_normalization_row_dict = {\n",
    "            \"custom_id\": f\"{row['id']}_norm\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": COMPLETION_ENDPOINT,\n",
    "            \"body\": {\n",
    "                \"model\": MODEL,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"developer\",\n",
    "                        \"content\": CLAIM_NORMALIZATION_SYSTEM_MESSAGE,\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Postagem: {row['text']}\\nDeclaração normalizada:\",\n",
    "                    },\n",
    "                ],\n",
    "                \"max_completion_tokens\": MAX_COMPLETION_TOKENS,\n",
    "                \"metadata\": {\n",
    "                    \"source\": row[\"source\"],\n",
    "                    \"subject\": row[\"subject\"],\n",
    "                    \"classificacao\": row[\"classificacao\"],\n",
    "                    \"task\": \"claim_normalization\",\n",
    "                },\n",
    "                \"verbosity\": VERBOSITY,\n",
    "                \"reasoning_effort\": REASONING_EFFORT,\n",
    "                \"temperature\": TEMPERATURE,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        batch_jsonl_rows.append(claim_extraction_row_dict)\n",
    "        batch_jsonl_rows.append(claim_normalization_row_dict)\n",
    "\n",
    "    return batch_jsonl_rows\n",
    "\n",
    "\n",
    "# Function to save JSONL file\n",
    "def save_batch_jsonl_file(batch_jsonl_rows, batch_file_path):\n",
    "    try:\n",
    "        with open(batch_file_path, \"w\") as jsonl_file:\n",
    "            for row in batch_jsonl_rows:\n",
    "                jsonl_file.write(\n",
    "                    json.dumps(row) + \"\\n\"\n",
    "                )  # Use json.dumps to format with double quotes\n",
    "        logging.info(f\"Saved batch to {batch_file_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving batch to {batch_file_path}: {e}\")\n",
    "\n",
    "\n",
    "# Function to upload file to OpenAI\n",
    "def upload_batch_file_to_openai(batch_file_path):\n",
    "    try:\n",
    "        batch_uploaded_file = client.files.create(\n",
    "            file=open(batch_file_path, \"rb\"), purpose=\"batch\"\n",
    "        )\n",
    "        logging.info(f\"Uploaded batch to OpenAI successfully! File ID: {batch_uploaded_file.id}\")\n",
    "        return batch_uploaded_file.id\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error uploading batch to OpenAI: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to create a batch in OpenAI\n",
    "def create_openai_batch(batch_input_file_id):\n",
    "    try:\n",
    "        batch_info = client.batches.create(\n",
    "            input_file_id=batch_input_file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\"description\": f\"Batch created from file ID {batch_input_file_id}\"},\n",
    "        )\n",
    "        logging.info(f\"Created batch successfully! Batch ID: {batch_info.id}\")\n",
    "        return batch_info.id\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating batch: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main batch processing loop\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "current_batch = 0\n",
    "\n",
    "for i in range(0, len(dataset_df), ROWS_PER_BATCH):\n",
    "    # Generate JSONL rows for the current batch\n",
    "    batch_df = dataset_df.iloc[i:i + ROWS_PER_BATCH]\n",
    "    batch_jsonl_rows = generate_batch_jsonl_rows(batch_df)\n",
    "\n",
    "    # Save the batch to a JSONL file\n",
    "    os.makedirs(UNPROCESSED_BATCHES_DIR, exist_ok=True)\n",
    "    batch_file_name = f\"batch_{current_batch}_{timestamp}.jsonl\"\n",
    "    batch_file_path = f\"{UNPROCESSED_BATCHES_DIR}/{batch_file_name}\"\n",
    "    save_batch_jsonl_file(batch_jsonl_rows, batch_file_path)\n",
    "\n",
    "    # Upload the batch file to OpenAI\n",
    "    batch_input_file_id = upload_batch_file_to_openai(batch_file_path)\n",
    "\n",
    "    if batch_input_file_id:\n",
    "        # Move the batch file to the uploaded directory\n",
    "        os.makedirs(UPLOADED_BATCHES_DIR, exist_ok=True)\n",
    "        uploaded_file_path = f\"{UPLOADED_BATCHES_DIR}/batch-file-id_{batch_input_file_id}_{batch_file_name}\"\n",
    "        move_file_to_directory(batch_file_path, uploaded_file_path)\n",
    "\n",
    "        # Create the batch in OpenAI\n",
    "        batch_id = create_openai_batch(batch_input_file_id)\n",
    "\n",
    "        if batch_id:\n",
    "            # Move the batch file to the processing directory\n",
    "            os.makedirs(PROCESSING_BATCHES_DIR, exist_ok=True)\n",
    "            processing_file_path = f\"{PROCESSING_BATCHES_DIR}/batch-id_{batch_id}_{batch_file_name}\"\n",
    "            move_file_to_directory(uploaded_file_path, processing_file_path)\n",
    "\n",
    "    # Increment the batch counter\n",
    "    current_batch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28dfbbe",
   "metadata": {},
   "source": [
    "### Check Batches Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bf3068d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 11:58:36,479 - INFO - HTTP Request: GET https://api.openai.com/v1/batches/batch_68d6a93fb65881909cdd3c6a4975073f \"HTTP/1.1 200 OK\"\n",
      "2025-09-26 11:58:36,482 - INFO - Batch batch_68d6a93fb65881909cdd3c6a4975073f status: in_progress\n"
     ]
    }
   ],
   "source": [
    "# Function to retrieve batch status from OpenAI\n",
    "def retrieve_batch_status(batch_id):\n",
    "    try:\n",
    "        batch_object = client.batches.retrieve(batch_id)\n",
    "        batch_status = batch_object.status\n",
    "\n",
    "        logging.info(f\"Batch {batch_id} status: {batch_status}\")\n",
    "\n",
    "        if batch_status in [\"completed\", \"failed\"]:\n",
    "            return batch_object\n",
    "        elif batch_status in [\"created\", \"in_progress\", \"finalizing\"]:\n",
    "            return None\n",
    "        else:\n",
    "            logging.warning(f\"Batch {batch_id} has unexpected status: {batch_status}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving batch {batch_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process completed batches\n",
    "def process_completed_batch(batch_id, batch_info, batch_processing_file):\n",
    "    error_occurred = False\n",
    "\n",
    "    if batch_info.output_file_id:\n",
    "        try:\n",
    "            results_response = client.files.content(batch_info.output_file_id)\n",
    "            results_dir = RESULTS_BATCHES_DIR\n",
    "            os.makedirs(results_dir, exist_ok=True)\n",
    "            completed_file_path = f\"{results_dir}/{os.path.splitext(batch_processing_file)[0]}_results.jsonl\"\n",
    "\n",
    "            with open(\n",
    "                completed_file_path, \"wb\"\n",
    "            ) as result_file:  # Use \"wb\" for binary write\n",
    "                result_file.write(\n",
    "                    results_response.read()\n",
    "                )  # Read binary content and write\n",
    "\n",
    "            logging.info(f\"Saved results for batch {batch_id} to {completed_file_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing completed batch {batch_id}: {e}\")\n",
    "            error_occurred = True\n",
    "\n",
    "    if batch_info.error_file_id:\n",
    "        try:\n",
    "            error_response = client.files.content(batch_info.error_file_id)\n",
    "            errors_dir = FAILED_BATCHES_DIR\n",
    "            os.makedirs(errors_dir, exist_ok=True)\n",
    "            failed_file_path = f\"{errors_dir}/{os.path.splitext(batch_processing_file)[0]}_errors.jsonl\"\n",
    "\n",
    "            with open(\n",
    "                failed_file_path, \"wb\"\n",
    "            ) as error_file:  # Use \"wb\" for binary write\n",
    "                error_file.write(error_response.read())  # Read binary content and write\n",
    "\n",
    "            logging.info(f\"Saved errors for batch {batch_id} to {failed_file_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing failed {batch_id}: {e}\")\n",
    "            error_occurred = True\n",
    "\n",
    "    if not error_occurred:\n",
    "        # Move the processing file to processed directory\n",
    "        os.makedirs(PROCESSED_BATCHES_DIR, exist_ok=True)\n",
    "        processed_file_path = f\"{PROCESSED_BATCHES_DIR}/{batch_processing_file}\"\n",
    "        move_file_to_directory(\n",
    "            f\"{PROCESSING_BATCHES_DIR}/{batch_processing_file}\", processed_file_path\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"Moved processing file for batch {batch_id} to processed directory\"\n",
    "        )\n",
    "\n",
    "# Function to get processing batches\n",
    "def get_processing_batches():\n",
    "    if not os.path.exists(PROCESSING_BATCHES_DIR):\n",
    "        logging.warning(f\"Processing directory {PROCESSING_BATCHES_DIR} does not exist.\")\n",
    "        return []\n",
    "\n",
    "    processing_file_paths = os.listdir(PROCESSING_BATCHES_DIR)\n",
    "    if not processing_file_paths:\n",
    "        logging.info(\"No batches are currently being processed.\")\n",
    "        return []\n",
    "\n",
    "    return processing_file_paths\n",
    "\n",
    "# Main function to check on batches being processed\n",
    "def check_batches_processing():\n",
    "    processing_file_paths = get_processing_batches()\n",
    "\n",
    "    if not processing_file_paths:\n",
    "        logging.info(\"No batches are currently being processed.\")\n",
    "        return\n",
    "\n",
    "    for batch_file in processing_file_paths:\n",
    "        try:\n",
    "            batch_id = f\"batch_{batch_file.split('_')[2]}\"\n",
    "            batch_info = retrieve_batch_status(batch_id)\n",
    "\n",
    "            if batch_info:\n",
    "                process_completed_batch(\n",
    "                    batch_id,\n",
    "                    batch_info,\n",
    "                    batch_file\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing batch file {batch_file}: {e}\")\n",
    "\n",
    "# Call the batches processing check function\n",
    "check_batches_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec472771",
   "metadata": {},
   "source": [
    "### Save Dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
