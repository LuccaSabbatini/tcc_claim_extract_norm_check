% bert-large-portuguese-cased - \textit{Claim Extraction} (GPT-5 nano) - fakebr 
\begin{table}[!htbp]
\centering
\begin{tabular}{l*4c}
\toprule
\multicolumn{5}{c}{\textbf{BERTimbau | Fake.br | \textit{Claim Extraction} (GPT-5 nano)}} \\ 
\midrule
\textbf{Métrica} & \textbf{Treino 1} & \textbf{Treino 2} & \textbf{Treino 3} & \textbf{Média} \\ 
\midrule
Época do melhor modelo & 2 & 2 & 2 & 2.000 \\ 
Perda no treinamento & 0.461 & 0.453 & 0.413 & 0.442 \\ 
Perda na validação & 0.470 & 0.504 & 0.480 & 0.485 \\ 
Acurácia & 0.781 & 0.749 & 0.770 & 0.767 \\ 
Precisão & 0.798 & 0.836 & 0.764 & 0.800 \\ 
F1 & 0.775 & 0.711 & 0.772 & 0.753 \\ 
Recall & 0.752 & 0.619 & 0.780 & 0.717 \\ 
\bottomrule
\end{tabular}
\caption{Métricas dos melhores modelos de cada ajuste-fino do \textit{Transformer} BERTimbau treinando na base Fake.br \textit{Claim Extraction} (GPT-5 nano).}
\end{table}

% bert-large-portuguese-cased - Original - fakebr 
\begin{table}[!htbp]
\centering
\begin{tabular}{l*4c}
\toprule
\multicolumn{5}{c}{\textbf{BERTimbau | Fake.br | Original}} \\ 
\midrule
\textbf{Métrica} & \textbf{Treino 1} & \textbf{Treino 2} & \textbf{Treino 3} & \textbf{Média} \\ 
\midrule
Época do melhor modelo & 5 & 2 & 2 & 3.000 \\ 
Perda no treinamento & 0.016 & 0.086 & 0.114 & 0.072 \\ 
Perda na validação & 0.059 & 0.050 & 0.074 & 0.061 \\ 
Acurácia & 0.985 & 0.986 & 0.977 & 0.983 \\ 
Precisão & 0.986 & 0.992 & 0.965 & 0.981 \\ 
F1 & 0.985 & 0.986 & 0.977 & 0.983 \\ 
Recall & 0.985 & 0.981 & 0.990 & 0.985 \\ 
\bottomrule
\end{tabular}
\caption{Métricas dos melhores modelos de cada ajuste-fino do \textit{Transformer} BERTimbau treinando na base Fake.br Original.}
\end{table}

% bert-large-portuguese-cased - \textit{Claim Normalization} (GPT-5 nano) - fakebr 
\begin{table}[!htbp]
\centering
\begin{tabular}{l*4c}
\toprule
\multicolumn{5}{c}{\textbf{BERTimbau | Fake.br | \textit{Claim Normalization} (GPT-5 nano)}} \\ 
\midrule
\textbf{Métrica} & \textbf{Treino 1} & \textbf{Treino 2} & \textbf{Treino 3} & \textbf{Média} \\ 
\midrule
Época do melhor modelo & 1 & 1 & 1 & 1.000 \\ 
Perda no treinamento & 0.527 & 0.521 & 0.560 & 0.536 \\ 
Perda na validação & 0.397 & 0.392 & 0.418 & 0.403 \\ 
Acurácia & 0.814 & 0.816 & 0.816 & 0.815 \\ 
Precisão & 0.891 & 0.889 & 0.902 & 0.894 \\ 
F1 & 0.793 & 0.796 & 0.793 & 0.794 \\ 
Recall & 0.714 & 0.721 & 0.708 & 0.714 \\ 
\bottomrule
\end{tabular}
\caption{Métricas dos melhores modelos de cada ajuste-fino do \textit{Transformer} BERTimbau treinando na base Fake.br \textit{Claim Normalization} (GPT-5 nano).}
\end{table}

% xlm-roberta-large - \textit{Claim Extraction} (GPT-5 nano) - fakebr 
\begin{table}[!htbp]
\centering
\begin{tabular}{l*4c}
\toprule
\multicolumn{5}{c}{\textbf{XLM-RoBERTa-Large | Fake.br | \textit{Claim Extraction} (GPT-5 nano)}} \\ 
\midrule
\textbf{Métrica} & \textbf{Treino 1} & \textbf{Treino 2} & \textbf{Treino 3} & \textbf{Média} \\ 
\midrule
Época do melhor modelo & 4 & 2 & 4 & 3.333 \\ 
Perda no treinamento & 0.494 & 0.607 & 0.367 & 0.489 \\ 
Perda na validação & 0.510 & 0.553 & 0.504 & 0.522 \\ 
Acurácia & 0.745 & 0.709 & 0.774 & 0.743 \\ 
Precisão & 0.723 & 0.790 & 0.762 & 0.758 \\ 
F1 & 0.757 & 0.662 & 0.779 & 0.733 \\ 
Recall & 0.794 & 0.570 & 0.797 & 0.720 \\ 
\bottomrule
\end{tabular}
\caption{Métricas dos melhores modelos de cada ajuste-fino do \textit{Transformer} XLM-RoBERTa-Large treinando na base Fake.br \textit{Claim Extraction} (GPT-5 nano).}
\end{table}

% xlm-roberta-large - Original - fakebr 
\begin{table}[!htbp]
\centering
\begin{tabular}{l*4c}
\toprule
\multicolumn{5}{c}{\textbf{XLM-RoBERTa-Large | Fake.br | Original}} \\ 
\midrule
\textbf{Métrica} & \textbf{Treino 1} & \textbf{Treino 2} & \textbf{Treino 3} & \textbf{Média} \\ 
\midrule
Época do melhor modelo & 2 & 4 & 2 & 2.667 \\ 
Perda no treinamento & 0.252 & 0.132 & 0.183 & 0.189 \\ 
Perda na validação & 0.159 & 0.104 & 0.098 & 0.120 \\ 
Acurácia & 0.951 & 0.974 & 0.973 & 0.966 \\ 
Precisão & 0.939 & 0.987 & 0.983 & 0.970 \\ 
F1 & 0.952 & 0.974 & 0.973 & 0.966 \\ 
Recall & 0.965 & 0.961 & 0.963 & 0.963 \\ 
\bottomrule
\end{tabular}
\caption{Métricas dos melhores modelos de cada ajuste-fino do \textit{Transformer} XLM-RoBERTa-Large treinando na base Fake.br Original.}
\end{table}

% xlm-roberta-large - \textit{Claim Normalization} (GPT-5 nano) - fakebr 
\begin{table}[!htbp]
\centering
\begin{tabular}{l*4c}
\toprule
\multicolumn{5}{c}{\textbf{XLM-RoBERTa-Large | Fake.br | \textit{Claim Normalization} (GPT-5 nano)}} \\ 
\midrule
\textbf{Métrica} & \textbf{Treino 1} & \textbf{Treino 2} & \textbf{Treino 3} & \textbf{Média} \\ 
\midrule
Época do melhor modelo & 1 & 1 & 2 & 1.333 \\ 
Perda no treinamento & 0.691 & 0.585 & 0.439 & 0.572 \\ 
Perda na validação & 0.693 & 0.453 & 0.374 & 0.507 \\ 
Acurácia & 0.501 & 0.783 & 0.846 & 0.710 \\ 
Precisão & 0.000 & 0.909 & 0.840 & 0.583 \\ 
F1 & 0.000 & 0.743 & 0.847 & 0.530 \\ 
Recall & 0.000 & 0.628 & 0.854 & 0.494 \\ 
\bottomrule
\end{tabular}
\caption{Métricas dos melhores modelos de cada ajuste-fino do \textit{Transformer} XLM-RoBERTa-Large treinando na base Fake.br \textit{Claim Normalization} (GPT-5 nano).}
\end{table}

% bert-large-portuguese-cased - \textit{Claim Extraction} (GPT-5 nano) - faketweetbr 
\begin{table}[!htbp]
\centering
\begin{tabular}{l*4c}
\toprule
\multicolumn{5}{c}{\textbf{BERTimbau | FakeTweet.BR | \textit{Claim Extraction} (GPT-5 nano)}} \\ 
\midrule
\textbf{Métrica} & \textbf{Treino 1} & \textbf{Treino 2} & \textbf{Treino 3} & \textbf{Média} \\ 
\midrule
Época do melhor modelo & 5 & 3 & 2 & 3.333 \\ 
Perda no treinamento & 0.193 & 0.448 & 0.559 & 0.400 \\ 
Perda na validação & 0.395 & 0.672 & 0.660 & 0.576 \\ 
Acurácia & 0.800 & 0.550 & 0.550 & 0.633 \\ 
Precisão & 0.833 & 0.667 & 0.588 & 0.696 \\ 
F1 & 0.833 & 0.571 & 0.690 & 0.698 \\ 
Recall & 0.833 & 0.500 & 0.833 & 0.722 \\ 
\bottomrule
\end{tabular}
\caption{Métricas dos melhores modelos de cada ajuste-fino do \textit{Transformer} BERTimbau treinando na base FakeTweet.BR \textit{Claim Extraction} (GPT-5 nano).}
\end{table}

% bert-large-portuguese-cased - \textit{Claim Extraction} (GPT-5) - faketweetbr 
\begin{table}[!htbp]
\centering
\begin{tabular}{l*4c}
\toprule
\multicolumn{5}{c}{\textbf{BERTimbau | FakeTweet.BR | \textit{Claim Extraction} (GPT-5)}} \\ 
\midrule
\textbf{Métrica} & \textbf{Treino 1} & \textbf{Treino 2} & \textbf{Treino 3} & \textbf{Média} \\ 
\midrule
Época do melhor modelo & 5 & 6 & 5 & 5.333 \\ 
Perda no treinamento & 0.065 & 0.102 & 0.133 & 0.100 \\ 
Perda na validação & 0.461 & 0.444 & 0.545 & 0.483 \\ 
Acurácia & 0.850 & 0.750 & 0.800 & 0.800 \\ 
Precisão & 0.800 & 0.818 & 0.833 & 0.817 \\ 
F1 & 0.889 & 0.783 & 0.833 & 0.835 \\ 
Recall & 1.000 & 0.750 & 0.833 & 0.861 \\ 
\bottomrule
\end{tabular}
\caption{Métricas dos melhores modelos de cada ajuste-fino do \textit{Transformer} BERTimbau treinando na base FakeTweet.BR \textit{Claim Extraction} (GPT-5).}
\end{table}

% bert-large-portuguese-cased - Original - faketweetbr 
\begin{table}[!htbp]
\centering
\begin{tabular}{l*4c}
\toprule
\multicolumn{5}{c}{\textbf{BERTimbau | FakeTweet.BR | Original}} \\ 
\midrule
\textbf{Métrica} & \textbf{Treino 1} & \textbf{Treino 2} & \textbf{Treino 3} & \textbf{Média} \\ 
\midrule
Época do melhor modelo & 5 & 5 & 5 & 5.000 \\ 
Perda no treinamento & 0.056 & 0.096 & 0.086 & 0.079 \\ 
Perda na validação & 0.504 & 0.466 & 0.549 & 0.506 \\ 
Acurácia & 0.800 & 0.800 & 0.750 & 0.783 \\ 
Precisão & 0.833 & 0.900 & 0.818 & 0.851 \\ 
F1 & 0.833 & 0.818 & 0.783 & 0.811 \\ 
Recall & 0.833 & 0.750 & 0.750 & 0.778 \\ 
\bottomrule
\end{tabular}
\caption{Métricas dos melhores modelos de cada ajuste-fino do \textit{Transformer} BERTimbau treinando na base FakeTweet.BR Original.}
\end{table}

% bert-large-portuguese-cased - \textit{Claim Normalization} (GPT-5 nano) - faketweetbr 
\begin{table}[!htbp]
\centering
\begin{tabular}{l*4c}
\toprule
\multicolumn{5}{c}{\textbf{BERTimbau | FakeTweet.BR | \textit{Claim Normalization} (GPT-5 nano)}} \\ 
\midrule
\textbf{Métrica} & \textbf{Treino 1} & \textbf{Treino 2} & \textbf{Treino 3} & \textbf{Média} \\ 
\midrule
Época do melhor modelo & 2 & 2 & 2 & 2.000 \\ 
Perda no treinamento & 0.564 & 0.557 & 0.579 & 0.566 \\ 
Perda na validação & 0.597 & 0.659 & 0.662 & 0.639 \\ 
Acurácia & 0.700 & 0.650 & 0.600 & 0.650 \\ 
Precisão & 0.667 & 0.632 & 0.600 & 0.633 \\ 
F1 & 0.800 & 0.774 & 0.750 & 0.775 \\ 
Recall & 1.000 & 1.000 & 1.000 & 1.000 \\ 
\bottomrule
\end{tabular}
\caption{Métricas dos melhores modelos de cada ajuste-fino do \textit{Transformer} BERTimbau treinando na base FakeTweet.BR \textit{Claim Normalization} (GPT-5 nano).}
\end{table}

% bert-large-portuguese-cased - \textit{Claim Normalization} (GPT-5) - faketweetbr 
\begin{table}[!htbp]
\centering
\begin{tabular}{l*4c}
\toprule
\multicolumn{5}{c}{\textbf{BERTimbau | FakeTweet.BR | \textit{Claim Normalization} (GPT-5)}} \\ 
\midrule
\textbf{Métrica} & \textbf{Treino 1} & \textbf{Treino 2} & \textbf{Treino 3} & \textbf{Média} \\ 
\midrule
Época do melhor modelo & 3 & 3 & 7 & 4.333 \\ 
Perda no treinamento & 0.349 & 0.352 & 0.078 & 0.260 \\ 
Perda na validação & 0.580 & 0.607 & 0.398 & 0.528 \\ 
Acurácia & 0.600 & 0.550 & 0.800 & 0.650 \\ 
Precisão & 0.667 & 0.615 & 0.786 & 0.689 \\ 
F1 & 0.667 & 0.640 & 0.846 & 0.718 \\ 
Recall & 0.667 & 0.667 & 0.917 & 0.750 \\ 
\bottomrule
\end{tabular}
\caption{Métricas dos melhores modelos de cada ajuste-fino do \textit{Transformer} BERTimbau treinando na base FakeTweet.BR \textit{Claim Normalization} (GPT-5).}
\end{table}

% xlm-roberta-large - \textit{Claim Extraction} (GPT-5 nano) - faketweetbr 
\begin{table}[!htbp]
\centering
\begin{tabular}{l*4c}
\toprule
\multicolumn{5}{c}{\textbf{XLM-RoBERTa-Large | FakeTweet.BR | \textit{Claim Extraction} (GPT-5 nano)}} \\ 
\midrule
\textbf{Métrica} & \textbf{Treino 1} & \textbf{Treino 2} & \textbf{Treino 3} & \textbf{Média} \\ 
\midrule
Época do melhor modelo & 3 & 3 & 2 & 2.667 \\ 
Perda no treinamento & 0.606 & 0.571 & 0.642 & 0.606 \\ 
Perda na validação & 0.736 & 0.665 & 0.684 & 0.695 \\ 
Acurácia & 0.600 & 0.600 & 0.600 & 0.600 \\ 
Precisão & 0.600 & 0.600 & 0.600 & 0.600 \\ 
F1 & 0.750 & 0.750 & 0.750 & 0.750 \\ 
Recall & 1.000 & 1.000 & 1.000 & 1.000 \\ 
\bottomrule
\end{tabular}
\caption{Métricas dos melhores modelos de cada ajuste-fino do \textit{Transformer} XLM-RoBERTa-Large treinando na base FakeTweet.BR \textit{Claim Extraction} (GPT-5 nano).}
\end{table}

% xlm-roberta-large - \textit{Claim Extraction} (GPT-5) - faketweetbr 
\begin{table}[!htbp]
\centering
\begin{tabular}{l*4c}
\toprule
\multicolumn{5}{c}{\textbf{XLM-RoBERTa-Large | FakeTweet.BR | \textit{Claim Extraction} (GPT-5)}} \\ 
\midrule
\textbf{Métrica} & \textbf{Treino 1} & \textbf{Treino 2} & \textbf{Treino 3} & \textbf{Média} \\ 
\midrule
Época do melhor modelo & 4 & 1 & 1 & 2.000 \\ 
Perda no treinamento & 0.606 & 0.641 & 0.679 & 0.642 \\ 
Perda na validação & 0.669 & 0.692 & 0.682 & 0.681 \\ 
Acurácia & 0.600 & 0.600 & 0.600 & 0.600 \\ 
Precisão & 0.600 & 0.600 & 0.600 & 0.600 \\ 
F1 & 0.750 & 0.750 & 0.750 & 0.750 \\ 
Recall & 1.000 & 1.000 & 1.000 & 1.000 \\ 
\bottomrule
\end{tabular}
\caption{Métricas dos melhores modelos de cada ajuste-fino do \textit{Transformer} XLM-RoBERTa-Large treinando na base FakeTweet.BR \textit{Claim Extraction} (GPT-5).}
\end{table}

% xlm-roberta-large - Original - faketweetbr 
\begin{table}[!htbp]
\centering
\begin{tabular}{l*4c}
\toprule
\multicolumn{5}{c}{\textbf{XLM-RoBERTa-Large | FakeTweet.BR | Original}} \\ 
\midrule
\textbf{Métrica} & \textbf{Treino 1} & \textbf{Treino 2} & \textbf{Treino 3} & \textbf{Média} \\ 
\midrule
Época do melhor modelo & 6 & 5 & 5 & 5.333 \\ 
Perda no treinamento & 0.248 & 0.339 & 0.222 & 0.270 \\ 
Perda na validação & 0.513 & 0.557 & 0.397 & 0.489 \\ 
Acurácia & 0.800 & 0.800 & 0.850 & 0.817 \\ 
Precisão & 0.900 & 0.786 & 0.909 & 0.865 \\ 
F1 & 0.818 & 0.846 & 0.870 & 0.845 \\ 
Recall & 0.750 & 0.917 & 0.833 & 0.833 \\ 
\bottomrule
\end{tabular}
\caption{Métricas dos melhores modelos de cada ajuste-fino do \textit{Transformer} XLM-RoBERTa-Large treinando na base FakeTweet.BR Original.}
\end{table}

% xlm-roberta-large - \textit{Claim Normalization} (GPT-5 nano) - faketweetbr 
\begin{table}[!htbp]
\centering
\begin{tabular}{l*4c}
\toprule
\multicolumn{5}{c}{\textbf{XLM-RoBERTa-Large | FakeTweet.BR | \textit{Claim Normalization} (GPT-5 nano)}} \\ 
\midrule
\textbf{Métrica} & \textbf{Treino 1} & \textbf{Treino 2} & \textbf{Treino 3} & \textbf{Média} \\ 
\midrule
Época do melhor modelo & 2 & 9 & 2 & 4.333 \\ 
Perda no treinamento & 0.640 & 0.065 & 0.674 & 0.459 \\ 
Perda na validação & 0.719 & 0.298 & 0.686 & 0.568 \\ 
Acurácia & 0.600 & 0.850 & 0.600 & 0.683 \\ 
Precisão & 0.600 & 1.000 & 0.600 & 0.733 \\ 
F1 & 0.750 & 0.857 & 0.750 & 0.786 \\ 
Recall & 1.000 & 0.750 & 1.000 & 0.917 \\ 
\bottomrule
\end{tabular}
\caption{Métricas dos melhores modelos de cada ajuste-fino do \textit{Transformer} XLM-RoBERTa-Large treinando na base FakeTweet.BR \textit{Claim Normalization} (GPT-5 nano).}
\end{table}

% xlm-roberta-large - \textit{Claim Normalization} (GPT-5) - faketweetbr 
\begin{table}[!htbp]
\centering
\begin{tabular}{l*4c}
\toprule
\multicolumn{5}{c}{\textbf{XLM-RoBERTa-Large | FakeTweet.BR | \textit{Claim Normalization} (GPT-5)}} \\ 
\midrule
\textbf{Métrica} & \textbf{Treino 1} & \textbf{Treino 2} & \textbf{Treino 3} & \textbf{Média} \\ 
\midrule
Época do melhor modelo & 4 & 1 & 6 & 3.667 \\ 
Perda no treinamento & 0.492 & 0.658 & 0.459 & 0.536 \\ 
Perda na validação & 0.626 & 0.671 & 0.562 & 0.620 \\ 
Acurácia & 0.600 & 0.600 & 0.700 & 0.633 \\ 
Precisão & 0.600 & 0.600 & 0.714 & 0.638 \\ 
F1 & 0.750 & 0.750 & 0.769 & 0.756 \\ 
Recall & 1.000 & 1.000 & 0.833 & 0.944 \\ 
\bottomrule
\end{tabular}
\caption{Métricas dos melhores modelos de cada ajuste-fino do \textit{Transformer} XLM-RoBERTa-Large treinando na base FakeTweet.BR \textit{Claim Normalization} (GPT-5).}
\end{table}

